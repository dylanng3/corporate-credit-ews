<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>validation_report</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="VALIDATION_REPORT_files/libs/clipboard/clipboard.min.js"></script>
<script src="VALIDATION_REPORT_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="VALIDATION_REPORT_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="VALIDATION_REPORT_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="VALIDATION_REPORT_files/libs/quarto-html/popper.min.js"></script>
<script src="VALIDATION_REPORT_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="VALIDATION_REPORT_files/libs/quarto-html/anchor.min.js"></script>
<link href="VALIDATION_REPORT_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="VALIDATION_REPORT_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="VALIDATION_REPORT_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="VALIDATION_REPORT_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="VALIDATION_REPORT_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="early-warning-system-ews-independent-validation-report" class="level1">
<h1>Early Warning System (EWS) – Independent Validation Report</h1>
<blockquote class="blockquote">
<p><strong>Document info</strong><br>
<strong>Model</strong>: EWS – Probability of Deterioration / Default (12m)<br>
<strong>Version</strong>: v1.0.0<br>
<strong>Date</strong>: October 24, 2025<br>
<strong>Owner (Model Dev)</strong>: Data Science Team<br>
<strong>Validator</strong>: Independent Model Risk Management<br>
<strong>Scope</strong>: Independent model validation for production approval</p>
</blockquote>
<hr>
<section id="executive-summary-1-page" class="level2">
<h2 class="anchored" data-anchor-id="executive-summary-1-page">1. Executive Summary (1 page)</h2>
<p><strong>Conclusion: PASS WITH CONDITIONS</strong></p>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<p><strong>Discrimination (Strong)</strong>: * AUC = <strong>0.823</strong> (95% CI: 0.813–0.833), KS = <strong>0.527</strong> * PR-AUC = <strong>0.155</strong> (11.3× baseline prevalence 1.37%) * Lift@10% = <strong>5.98×</strong> (top decile captures 60% of defaults) * <strong>Status</strong>: Stable over 18 months, no degradation (rolling 12M AUC: 0.819–0.834)</p>
<p><strong>Calibration (Acceptable)</strong>: * Brier = <strong>1.26%</strong> (rolling 12M avg), median decile error = <strong>12.8 bp</strong> * Mild over-prediction in deciles 2-3 (+14 bp), under-prediction in decile 10 (+91 bp) * No systemic bias in deciles 4-9</p>
<p><strong>Stability &amp; Drift (Synthetic Limitation)</strong>: * PSI(score) = <strong>0.00</strong> across all months (<strong>artificially perfect</strong> due to synthetic data) * <strong>Production validation required</strong>: Establish realistic PSI baseline using first 3 months of real data * Recalibration trigger: PSI &gt; 0.25 (mandatory), PSI &gt; 0.10 (watch)</p>
<p><strong>Thresholds &amp; Operational Fitness (Feasible)</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 21%">
<col style="width: 19%">
<col style="width: 16%">
<col style="width: 5%">
</colgroup>
<thead>
<tr class="header">
<th>Tier</th>
<th>PD Threshold</th>
<th>Alert Rate</th>
<th>Precision (95% CI)</th>
<th>Recall (95% CI)</th>
<th>Alerts/month</th>
<th>FTE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Amber</strong></td>
<td>2.0%</td>
<td>8.3%</td>
<td>9.6% (9.1–10.1%)</td>
<td>57.5% (55.7–59.3%)</td>
<td>830</td>
<td>~5</td>
</tr>
<tr class="even">
<td><strong>Red</strong></td>
<td>5.0%</td>
<td>4.2%</td>
<td>16.3% (14.8–17.9%)</td>
<td>48.2% (46.3–50.1%)</td>
<td>421</td>
<td>~10</td>
</tr>
<tr class="odd">
<td><strong>Combined</strong></td>
<td>—</td>
<td>8.3%</td>
<td>—</td>
<td>57.5%</td>
<td><strong>830 (unique)</strong></td>
<td><strong>~15</strong></td>
</tr>
</tbody>
</table>
<ul>
<li>✓ Red ⊆ Amber (all 421 red alerts within 830 amber, combined recall = amber recall)</li>
<li>✓ Workload feasible: 15 FTE vs.&nbsp;20 FTE capacity</li>
<li>⚠️ 42.5% uncaptured defaults (1 − recall@amber)</li>
</ul>
<p><strong>Risks &amp; Mitigations</strong>: 1. <strong>Synthetic data</strong> lacks real-world drift → <strong>6-month pilot</strong> with weekly monitoring 2. <strong>Low precision</strong> (Red 16%, Amber 10%) → high false positives → <strong>threshold review Q2 2026</strong> 3. <strong>42.5% uncaptured defaults</strong> → complement with quarterly credit reviews 4. <strong>No segment validation</strong> → <strong>segment analysis within 3 months</strong> of production</p>
<p><strong>Stress Resilience (Moderate)</strong>: * Combined +20% feature shock → AUC −2.8 pp (0.795, still &gt; 0.75 threshold), alert volume +25% (1,040/month, within 1,500 capacity)</p>
</section>
<section id="conditions-for-production-approval" class="level3">
<h3 class="anchored" data-anchor-id="conditions-for-production-approval">Conditions for Production Approval</h3>
<ol type="1">
<li><strong>6-month monitoring pilot</strong> (Nov 2025 – Apr 2026): Weekly AUC/PSI checks; validation gate at 3 months (Feb 2026)</li>
<li><strong>Mandatory recalibration trigger</strong>: AUC_roll12 &lt; 0.75 OR PSI &gt; 0.25 OR Precision@red &lt; 12% (2+ months)</li>
<li><strong>Threshold review</strong> (Q2 2026): Re-optimize Amber/Red using 6 months of production feedback</li>
<li><strong>Segment analysis</strong> (within 3 months): Stratify by sector/grade; implement differentiated thresholds if AUC variance &gt; 0.05</li>
</ol>
</section>
<section id="go-live-recommendation" class="level3">
<h3 class="anchored" data-anchor-id="go-live-recommendation">Go-Live Recommendation</h3>
<ul>
<li><strong>Phase 1</strong> (Nov 2025): Pilot shadow mode (20K customers, alerts observed not actioned)</li>
<li><strong>Phase 2</strong> (Feb 2026): Soft launch conditional production (50K customers, human-in-loop approval)</li>
<li><strong>Phase 3</strong> (May 2026): Full production rollout (200K+ customers) <strong>IF</strong> validation gates passed</li>
</ul>
<p><strong>Monitoring Owner</strong>: Data Science Team (model performance), Credit Risk Committee (business outcomes)</p>
<hr>
</section>
</section>
<section id="data-methodology-0.5-page" class="level2">
<h2 class="anchored" data-anchor-id="data-methodology-0.5-page">2. Data &amp; Methodology (0.5 page)</h2>
<section id="population-target" class="level3">
<h3 class="anchored" data-anchor-id="population-target">2.1 Population &amp; Target</h3>
<ul>
<li><strong>Population</strong>: Corporate credit customers (all sectors, grades A–G, EAD &gt; $0), single jurisdiction</li>
<li><strong>Sample</strong>: 180,000 observations (18 monthly cohorts × 10,000 customers, Jan 2024 – Jun 2025)</li>
<li><strong>Target</strong>: <code>y_event_12m</code> (12-month default: 90+ DPD or credit loss event), observed prevalence = <strong>1.37%</strong> (137 defaults/10,000 cohort)</li>
<li><strong>Observation window</strong>: Monthly snapshot (as-of date)</li>
<li><strong>Performance window</strong>: 12 months forward-looking</li>
<li><strong>Leakage controls</strong>: ✓ Timestamp checks (as-of &lt; outcome+12m), ✓ No forward-looking features, ✓ Label maturity (last 12m excluded)</li>
</ul>
</section>
<section id="model-calibration" class="level3">
<h3 class="anchored" data-anchor-id="model-calibration">2.2 Model &amp; Calibration</h3>
<ul>
<li><strong>Model</strong>: LightGBM binary classifier (20 features: financial ratios, behavioral, working capital)
<ul>
<li>Hyperparameters: <code>num_leaves=31</code>, <code>max_depth=5</code>, <code>learning_rate=0.05</code>, <code>class_weight='balanced'</code></li>
<li>Top 3 features: <code>dpd_max_180d</code>, <code>debt_to_ebitda</code>, <code>icr_ttm</code> (45% model variance)</li>
</ul></li>
<li><strong>Calibration</strong>: Isotonic regression, PD clipping [1e-6, 0.5]</li>
<li><strong>Explainability</strong>: ✓ SHAP reason codes (global + per-customer top 3 drivers)</li>
</ul>
</section>
<section id="data-quality-synthetic-limitation" class="level3">
<h3 class="anchored" data-anchor-id="data-quality-synthetic-limitation">2.3 Data Quality (Synthetic Limitation)</h3>
<ul>
<li><strong>Missingness</strong>: 0% (synthetic), <strong>production validation required</strong> for real data (missing rate &lt; 10% threshold)</li>
<li><strong>Outliers</strong>: Z-score normalization by sector size</li>
<li>⚠️ <strong>Limitation</strong>: Synthetic cohort has perfect data quality by design (no real-world delays, measurement errors, drift). First 3 months of production data critical for realistic validation.</li>
</ul>
<hr>
</section>
</section>
<section id="performance-results-1.5-pages" class="level2">
<h2 class="anchored" data-anchor-id="performance-results-1.5-pages">3. Performance Results (1.5 pages)</h2>
<section id="discrimination" class="level3">
<h3 class="anchored" data-anchor-id="discrimination">5.1 Discrimination</h3>
<section id="auc-area-under-roc-curve" class="level4">
<h4 class="anchored" data-anchor-id="auc-area-under-roc-curve">5.1.1 AUC (Area Under ROC Curve)</h4>
<ul>
<li><strong>Monthly AUC</strong>: Mean = <strong>0.823</strong>, Range = [0.792, 0.859]
<ul>
<li><strong>Best 3 months</strong>: 2024-08 (0.859), 2024-04 (0.852), 2024-02 (0.844)</li>
<li><strong>Worst 3 months</strong>: 2024-05 (0.792), 2024-10 (0.803), 2025-01 (0.801)</li>
</ul></li>
<li><strong>Rolling 12M AUC</strong>: Mean = <strong>0.827</strong> ± 0.006 (±1 SD), Range = [0.819, 0.834]
<ul>
<li><strong>Stable trend</strong> (no degradation observed over 18 months)</li>
<li>95% CI (DeLong method): 0.813–0.833 (pooled across all months)</li>
</ul></li>
<li><strong>Interpretation</strong>: <strong>Strong discrimination</strong> (AUC &gt; 0.8 meets industry standard for EWS). Model reliably ranks high-risk customers above low-risk.</li>
</ul>
</section>
<section id="ks-kolmogorov-smirnov-statistic" class="level4">
<h4 class="anchored" data-anchor-id="ks-kolmogorov-smirnov-statistic">5.1.2 KS (Kolmogorov-Smirnov Statistic)</h4>
<ul>
<li><strong>Monthly KS</strong>: Mean = <strong>0.527</strong>, Range = [0.465, 0.599]
<ul>
<li><strong>Best 3 months</strong>: 2024-08 (0.599), 2024-04 (0.572), 2024-02 (0.572)</li>
<li><strong>Worst 3 months</strong>: 2024-05 (0.466), 2024-10 (0.475), 2025-01 (0.494)</li>
</ul></li>
<li><strong>Rolling 12M KS</strong>: Mean = <strong>0.534</strong> ± 0.010, Range = [0.520, 0.549]</li>
<li><strong>Interpretation</strong>: <strong>Strong separation</strong> (KS &gt; 0.5 indicates good separation between default/non-default distributions). Typically KS ≈ 0.4–0.6 for credit PD models.</li>
</ul>
</section>
</section>
<section id="calibration" class="level3">
<h3 class="anchored" data-anchor-id="calibration">5.2 Calibration</h3>
<section id="brier-score-overall-calibration" class="level4">
<h4 class="anchored" data-anchor-id="brier-score-overall-calibration">5.2.1 Brier Score (Overall Calibration)</h4>
<ul>
<li><strong>Monthly Brier</strong>: Mean = <strong>1.26%</strong>, Range = [1.06%, 1.44%]
<ul>
<li><strong>Best 3 months</strong>: 2024-03 (1.07%), 2024-06 (1.17%), 2025-06 (1.17%)</li>
<li><strong>Worst 3 months</strong>: 2025-02 (1.44%), 2024-12 (1.30%), 2025-01 (1.31%)</li>
</ul></li>
<li><strong>Rolling 12M Brier</strong>: Mean = <strong>1.26%</strong> ± 0.02%, Range = [1.21%, 1.27%]</li>
<li><strong>Interpretation</strong>: <strong>Excellent calibration</strong> (Brier &lt; 2% is strong for rare-event models). Lower Brier = better probabilistic accuracy.</li>
</ul>
</section>
<section id="decile-calibration-granular-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="decile-calibration-granular-accuracy">5.2.2 Decile Calibration (Granular Accuracy)</h4>
<p><strong>Aggregate decile calibration</strong> (pooled across 18 months, N=180,000):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 10%">
<col style="width: 38%">
<col style="width: 21%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th>Decile</th>
<th>Count</th>
<th>Avg PD</th>
<th>Observed Default Rate (ODR)</th>
<th>Abs Error (bp)</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>18,000</td>
<td>0.056%</td>
<td>0.078%</td>
<td>+2.2 bp</td>
<td>✓ OK</td>
</tr>
<tr class="even">
<td>2</td>
<td>18,000</td>
<td>0.239%</td>
<td>0.117%</td>
<td>−12.2 bp</td>
<td>⚠️ Over-prediction</td>
</tr>
<tr class="odd">
<td>3</td>
<td>18,000</td>
<td>0.361%</td>
<td>0.472%</td>
<td>+11.1 bp</td>
<td>⚠️ Under-prediction</td>
</tr>
<tr class="even">
<td>4</td>
<td>18,000</td>
<td>0.464%</td>
<td>0.472%</td>
<td>+0.8 bp</td>
<td>✓ OK</td>
</tr>
<tr class="odd">
<td>5</td>
<td>18,000</td>
<td>0.557%</td>
<td>0.628%</td>
<td>+7.1 bp</td>
<td>✓ OK</td>
</tr>
<tr class="even">
<td>6</td>
<td>18,000</td>
<td>0.648%</td>
<td>0.489%</td>
<td>−15.9 bp</td>
<td>⚠️ Over-prediction</td>
</tr>
<tr class="odd">
<td>7</td>
<td>18,000</td>
<td>0.756%</td>
<td>0.944%</td>
<td>+18.8 bp</td>
<td>⚠️ Under-prediction</td>
</tr>
<tr class="even">
<td>8</td>
<td>18,000</td>
<td>0.893%</td>
<td>0.856%</td>
<td>−3.7 bp</td>
<td>✓ OK</td>
</tr>
<tr class="odd">
<td>9</td>
<td>18,000</td>
<td>1.173%</td>
<td>0.917%</td>
<td>−25.6 bp</td>
<td>⚠️ Over-prediction</td>
</tr>
<tr class="even">
<td>10</td>
<td>18,000</td>
<td>7.92%</td>
<td>8.83%</td>
<td>+91.1 bp</td>
<td>⚠️ Under-prediction</td>
</tr>
</tbody>
</table>
<p><strong>Observations</strong>: * <strong>Median absolute error: 12.8 bp</strong> (acceptable for EWS; industry tolerance typically ±20 bp) * <strong>Decile 10 (high-risk)</strong>: Under-predicts by +91 bp (PD=7.92% vs.&nbsp;ODR=8.83%) – suggests <strong>model slightly conservative at tail risk</strong>. This is <strong>acceptable</strong> for EWS (false negatives &gt; false positives in high-risk segment). * <strong>Deciles 2, 6, 9</strong>: Mild over-prediction (−12 to −26 bp) – may lead to slightly elevated false positive alerts, but within tolerance. * <strong>No systemic bias</strong> in deciles 4-5, 8 (errors &lt; 10 bp).</p>
<p><strong>Evidence</strong>: <code>monthly_calibration.csv</code> (182 rows, 10 deciles × 18 months + aggregate).</p>
</section>
</section>
<section id="imbalance-metric-pr-auc" class="level3">
<h3 class="anchored" data-anchor-id="imbalance-metric-pr-auc">5.3 Imbalance Metric (PR-AUC)</h3>
<ul>
<li><strong>Monthly PR-AUC</strong>: Mean = <strong>0.155</strong>, Range = [0.098, 0.207]
<ul>
<li><strong>Best 3 months</strong>: 2024-09 (0.207), 2024-08 (0.198), 2024-02 (0.171)</li>
<li><strong>Worst 3 months</strong>: 2024-10 (0.098), 2024-11 (0.130), 2024-06 (0.139)</li>
</ul></li>
<li><strong>Rolling 12M PR-AUC</strong>: Mean = <strong>0.156</strong> ± 0.007, Range = [0.148, 0.167]</li>
<li><strong>Baseline prevalence</strong>: 1.37% (random classifier PR-AUC ≈ 0.0137)</li>
<li><strong>Interpretation</strong>: <strong>11.3× baseline improvement</strong> (0.155 / 0.0137). PR-AUC emphasizes precision/recall trade-off for rare events (more relevant than AUC for imbalanced data).</li>
</ul>
</section>
<section id="lift-concentration" class="level3">
<h3 class="anchored" data-anchor-id="lift-concentration">5.4 Lift / Concentration</h3>
<ul>
<li><strong>Lift@10%</strong>: Mean = <strong>5.98×</strong>, Range = [5.11×, 6.64×]
<ul>
<li>Captures <strong>5.98× more defaults</strong> in top 10% scored customers vs.&nbsp;random selection</li>
<li>Top decile captures ~60% of all defaults on average</li>
</ul></li>
<li><strong>Lift@20%</strong>: Mean = <strong>3.49×</strong>, Range = [3.17×, 3.86×]
<ul>
<li>Top 2 deciles capture ~70% of all defaults</li>
</ul></li>
<li><strong>Event capture</strong>:
<ul>
<li>Top 10%: ~60% of defaults</li>
<li>Top 20%: ~70% of defaults</li>
<li>Top 30% (≈ Amber threshold @ 2% PD): ~<strong>57.5% of defaults</strong> (= Amber recall)</li>
</ul></li>
</ul>
<p><strong>Interpretation</strong>: <strong>Strong concentration</strong> (60% of defaults in top 10% indicates highly effective ranking). Operationally feasible for alert-based workflow (review top decile = 1,000 customers/month to catch 60% of risk).</p>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: <code>monthly_metrics.csv</code> (columns: lift_10pct, lift_20pct), <code>plot_lift.png</code>.</p>
</blockquote>
<hr>
</section>
</section>
<section id="stability-drift" class="level2">
<h2 class="anchored" data-anchor-id="stability-drift">6. Stability &amp; Drift</h2>
<section id="psi-population-stability-index" class="level3">
<h3 class="anchored" data-anchor-id="psi-population-stability-index">6.1 PSI (Population Stability Index)</h3>
<ul>
<li><strong>PSI(score) = 0.00</strong> across all 18 months (baseline: Jan 2024) due to synthetic cohort design.</li>
<li><strong>Feature PSI = 0.00</strong> for all monitored features (artificially perfect, no real-world covariate shift).</li>
<li><strong>⚠️ Production validation required</strong>: Establish realistic baseline PSI using <strong>first 3 months of production data</strong>. Recalibration trigger: PSI &gt; 0.25 (mandatory), PSI &gt; 0.10 (watch).</li>
</ul>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: <code>psi_monthly.csv</code>, <code>plot_psi.png</code>. Full PSI methodology and synthetic data limitations in <strong>Appendix F</strong>.</p>
</blockquote>
</section>
<section id="performance-drift" class="level3">
<h3 class="anchored" data-anchor-id="performance-drift">6.2 Performance Drift</h3>
<p><strong>AUC trend</strong> (rolling 12M): 0.834 (Q1’24) → 0.819 (Q2’25), stable drift −1.5 pp within expected variance. No statistically significant degradation (95% CI overlaps across all quarters). <strong>Recalibration not triggered</strong> (AUC_roll12 &gt; 0.75 threshold consistently).</p>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: <code>monthly_metrics.csv</code> (auc_roll12 column), <code>plot_performance_time.png</code> (with 95% CI bands).</p>
</blockquote>
<hr>
</section>
</section>
<section id="thresholds-operational-fitness" class="level2">
<h2 class="anchored" data-anchor-id="thresholds-operational-fitness">7. Thresholds &amp; Operational Fitness</h2>
<section id="threshold-selection-kpis" class="level3">
<h3 class="anchored" data-anchor-id="threshold-selection-kpis">7.1 Threshold Selection &amp; KPIs</h3>
<p><strong>Selected thresholds</strong> (v1.0): Amber = <strong>2.0%</strong> PD, Red = <strong>5.0%</strong> PD (Red ⊆ Amber).</p>
<p><strong>Performance at selected thresholds</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 14%">
<col style="width: 23%">
<col style="width: 20%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Tier</th>
<th>Alert Rate</th>
<th>Precision (95% CI)</th>
<th>Recall (95% CI)</th>
<th>Alerts/month</th>
<th>FTE Workload</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Amber (2%)</strong></td>
<td>8.3%</td>
<td>9.6% (9.1–10.1%)</td>
<td>57.5% (55.7–59.3%)</td>
<td>830</td>
<td>~5 FTE (2h/alert)</td>
</tr>
<tr class="even">
<td><strong>Red (5%)</strong></td>
<td>4.2%</td>
<td>16.3% (14.8–17.9%)</td>
<td>48.2% (46.3–50.1%)</td>
<td>421</td>
<td>~10 FTE (4h/alert)</td>
</tr>
<tr class="odd">
<td><strong>Combined (unique)</strong></td>
<td>8.3%</td>
<td>—</td>
<td>57.5%</td>
<td><strong>830</strong></td>
<td><strong>~15 FTE total</strong></td>
</tr>
</tbody>
</table>
<p><strong>Operational assessment</strong>: * ✓ Alert volume <strong>feasible</strong>: 830/month within capacity (1,000–1,500 alerts/month, team = 20 FTE) * ✓ Red ⊆ Amber: All 421 red alerts are subset of 830 amber (no additional customers) * ⚠️ Low precision: Amber 9.6%, Red 16.3% → high false positives (typical for EWS prevention-focused approach) * ⚠️ 42.5% uncaptured defaults (1 − recall@amber) rely on standard credit monitoring</p>
<p><strong>Threshold trade-offs</strong> (reference points):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 18%">
<col style="width: 13%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Threshold</th>
<th>Alert Rate</th>
<th>Precision</th>
<th>Recall</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.5%</td>
<td>10.4%</td>
<td>8.5%</td>
<td>60.7%</td>
<td>Broader coverage, more false positives</td>
</tr>
<tr class="even">
<td><strong>2.0% (Amber)</strong></td>
<td><strong>8.3%</strong></td>
<td><strong>9.6%</strong></td>
<td><strong>57.5%</strong></td>
<td><strong>Selected balance</strong></td>
</tr>
<tr class="odd">
<td>3.0%</td>
<td>5.9%</td>
<td>13.2%</td>
<td>53.8%</td>
<td>Tighter, lower recall</td>
</tr>
<tr class="even">
<td><strong>5.0% (Red)</strong></td>
<td><strong>4.2%</strong></td>
<td><strong>16.3%</strong></td>
<td><strong>48.2%</strong></td>
<td><strong>Selected escalation tier</strong></td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: <code>threshold_sweep.csv</code> (full 360-row sweep in <strong>Appendix D</strong>), <code>plot_alert_performance.png</code>, <code>plot_threshold_tradeoff.png</code>.</p>
</blockquote>
</section>
<section id="segment-specific-performance" class="level3">
<h3 class="anchored" data-anchor-id="segment-specific-performance">7.3 Segment-Specific Performance</h3>
<p><strong>⚠️ NOT VALIDATED</strong> – segment analysis not executed in backtest (illustrative data only). <strong>Production validation required</strong>: Complete segment stratification (by sector, grade, EAD) within <strong>3 months</strong> using real operational data. If AUC variance by segment &gt; 0.05, implement differentiated thresholds.</p>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: Full illustrative segment breakdowns in <strong>Appendix C</strong>.</p>
</blockquote>
<hr>
</section>
</section>
<section id="stress-sensitivity-testing" class="level2">
<h2 class="anchored" data-anchor-id="stress-sensitivity-testing">8. Stress / Sensitivity Testing</h2>
<section id="combined-stress-scenario" class="level3">
<h3 class="anchored" data-anchor-id="combined-stress-scenario">8.1 Combined Stress Scenario</h3>
<p><strong>Scenario</strong>: +20% shock on all key features (debt_to_ebitda, icr_ttm, dpd_max_180d, %util_mean_60d) simulating economic downturn.</p>
<p><strong>Impact</strong>: * <strong>ΔAUC</strong>: −2.8 pp (0.823 → 0.795, still &gt; 0.75 acceptable threshold) * <strong>ΔPrecision@red</strong>: −4.5 pp (16.3% → 11.8%, within tolerance) * <strong>ΔAlert volume</strong>: +210 alerts/month (+25%, from 830 → 1,040, still &lt; 1,500 capacity)</p>
<p><strong>Interpretation</strong>: <strong>Moderate resilience</strong> under stress. Alert volume remains operationally feasible; precision degrades but tolerable for EWS use case.</p>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: <code>stress_results.csv</code>, <code>STRESS_TEST_NOTE.md</code>. Feature-level sensitivity details in <strong>Appendix G</strong>.</p>
</blockquote>
<hr>
</section>
</section>
<section id="compliance-risk-limitations" class="level2">
<h2 class="anchored" data-anchor-id="compliance-risk-limitations">9. Compliance, Risk &amp; Limitations</h2>
<section id="model-limitations-critical" class="level3">
<h3 class="anchored" data-anchor-id="model-limitations-critical">9.1 Model Limitations (Critical)</h3>
<ol type="1">
<li><strong>Synthetic data bias</strong>:
<ul>
<li>Backtest uses <strong>perfect synthetic cohorts</strong> (no missingness, no outliers, no data delays).</li>
<li><strong>Risk</strong>: Production performance may degrade when encountering real-world data quality issues.</li>
<li><strong>Mitigation</strong>: 6-month pilot with <strong>weekly data quality monitoring</strong> (missing rate, outlier %, latency).</li>
</ul></li>
<li><strong>Low precision (high false positives)</strong>:
<ul>
<li>Amber precision = 9.6% → <strong>90% false positive rate</strong> (9 benign alerts per default).</li>
<li>Red precision = 16.3% → <strong>84% false positive rate</strong> (5 benign alerts per default).</li>
<li><strong>Risk</strong>: Alert fatigue, analyst workload, customer friction (unnecessary credit reviews).</li>
<li><strong>Mitigation</strong>: (i) Threshold recalibration by Q2 2026 based on feedback; (ii) introduce <strong>auto-dismiss rules</strong> for low-risk false positives (e.g., Amber + Grade A → manual review opt-out).</li>
</ul></li>
<li><strong>42.5% uncaptured defaults</strong>:
<ul>
<li>Recall@amber = 57.5% → <strong>42.5% of defaults not flagged</strong> (58/137 defaults/month).</li>
<li><strong>Risk</strong>: Silent deterioration in non-alerted customers (standard credit monitoring must catch).</li>
<li><strong>Mitigation</strong>: EWS is <strong>first line of defense</strong>, not sole monitoring tool. Complement with quarterly credit reviews for all customers.</li>
</ul></li>
<li><strong>No segment-specific thresholds</strong>:
<ul>
<li>Uniform 2%/5% thresholds across all sectors/grades.</li>
<li><strong>Risk</strong>: Over-alert in low-risk segments (Finance/Grade A), under-alert in high-risk (Manufacturing/Grade F).</li>
<li><strong>Mitigation</strong>: Segment analysis within 3 months; implement differentiated thresholds by Q3 2026 if performance gap &gt; 5 pp AUC.</li>
</ul></li>
<li><strong>Missing features in synthetic data</strong>:
<ul>
<li><code>limit_breach_cnt_90d</code>, <code>covenant_breach_cnt_180d</code> → PSI = NaN (not generated).</li>
<li><strong>Risk</strong>: Model relies on these features (rank #4, #6 in SHAP importance), but untested in backtest.</li>
<li><strong>Mitigation</strong>: Production validation must confirm feature availability and quality (no missing &gt; 10% threshold).</li>
</ul></li>
<li><strong>No temporal lead-time analysis</strong>:
<ul>
<li>Model predicts 12-month default, but <strong>early warning lead time not measured</strong> (e.g., how many months before default does alert trigger?).</li>
<li><strong>Risk</strong>: Alerts may fire too late (e.g., 1 month before default vs.&nbsp;6 months).</li>
<li><strong>Mitigation</strong>: Post-production, analyze <strong>alert → default time distribution</strong> to quantify intervention window.</li>
</ul></li>
</ol>
</section>
<section id="fairness-bias-not-applicable" class="level3">
<h3 class="anchored" data-anchor-id="fairness-bias-not-applicable">9.2 Fairness &amp; Bias (Not Applicable)</h3>
<ul>
<li><strong>Consumer protection</strong>: N/A (corporate credit model, not retail)</li>
<li><strong>Protected classes</strong>: N/A (business entities, not individuals)</li>
<li><strong>Adverse action</strong>: Alerts trigger internal review, <strong>not automatic limit reduction</strong> (human-in-the-loop). Compliant with fair lending principles.</li>
</ul>
</section>
<section id="known-operational-limits" class="level3">
<h3 class="anchored" data-anchor-id="known-operational-limits">9.3 Known Operational Limits</h3>
<ul>
<li><strong>Data delays</strong>: Model requires <strong>T+5 business day</strong> data availability (financial statements, transaction data). Alerts delayed if data late.</li>
<li><strong>Seasonality</strong>: No seasonal adjustment (e.g., Q4 retail spike, Q1 construction lull). Monitor for seasonal PSI spikes.</li>
<li><strong>Policy overrides</strong>: Credit officers can <strong>override alerts</strong> (e.g., temporary cashflow stress, customer explanation). Override logging required for governance.</li>
</ul>
</section>
<section id="governance-controls" class="level3">
<h3 class="anchored" data-anchor-id="governance-controls">9.4 Governance Controls</h3>
<ul>
<li><strong>Cut-off governance</strong>:
<ul>
<li>Thresholds (2%/5%) <strong>locked</strong> until Q2 2026 review.</li>
<li>Changes require MRM approval + backtest validation.</li>
</ul></li>
<li><strong>Override logging</strong>:
<ul>
<li>All alert overrides logged in <code>alert_decisions</code> table (customer_id, alert_tier, decision, reason, analyst_id, timestamp).</li>
<li>Monthly override report to Risk Committee (% overridden by tier, false positive rate post-review).</li>
</ul></li>
<li><strong>Challenger model</strong>:
<ul>
<li><strong>Logistic regression baseline</strong> deployed in parallel (6-month window).</li>
<li>Compare LightGBM vs.&nbsp;Logistic on AUC/precision/recall; switch if challenger outperforms by &gt;3 pp AUC consistently.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="monitoring-plan-triggers" class="level2">
<h2 class="anchored" data-anchor-id="monitoring-plan-triggers">10. Monitoring Plan &amp; Triggers</h2>
<section id="monthly-monitoring-metrics" class="level3">
<h3 class="anchored" data-anchor-id="monthly-monitoring-metrics">10.1 Monthly Monitoring Metrics</h3>
<p><strong>Discrimination</strong>: * AUC (monthly + rolling 12M): Target ≥ 0.80 (acceptable ≥ 0.75) * KS (monthly + rolling 12M): Target ≥ 0.50 (acceptable ≥ 0.40) * PR-AUC (monthly + rolling 12M): Target ≥ 0.15 (10× baseline)</p>
<p><strong>Calibration</strong>: * Brier score: Target ≤ 1.5% (acceptable ≤ 2.0%) * Decile calibration: Max |PD−ODR| &lt; 20 bp per decile</p>
<p><strong>Stability</strong>: * PSI (score): Target &lt; 0.10 (watch 0.10–0.25, severe &gt; 0.25) * PSI (top 10 features): Target &lt; 0.10 each (watch 0.10–0.25)</p>
<p><strong>Operational</strong>: * Alert volume (total): Target 800–1,200/month (capacity 1,500, critical &gt; 1,500) * Precision@red: Target ≥ 15% (acceptable ≥ 12%) * Recall@amber: Target ≥ 55% (acceptable ≥ 50%) * Override rate: Target &lt; 30% (acceptable &lt; 40%)</p>
</section>
<section id="recalibration-triggers-mandatory" class="level3">
<h3 class="anchored" data-anchor-id="recalibration-triggers-mandatory">10.2 Recalibration Triggers (Mandatory)</h3>
<p><strong>Immediate recalibration required if ANY</strong>: 1. <strong>AUC_roll12 &lt; 0.75</strong> for 2+ consecutive months 2. <strong>95% CI for AUC includes 0.75 or below</strong> (statistical significance) 3. <strong>Brier score &gt; 2.0%</strong> for 2+ consecutive months 4. <strong>PSI &gt; 0.25</strong> on <strong>any</strong> feature (severe drift) 5. <strong>Alert rate changes by &gt; 50%</strong> vs.&nbsp;baseline (e.g., amber jumps from 8% → 12%+ or drops to &lt; 4%) 6. <strong>Precision@red &lt; 12%</strong> for 2+ consecutive months</p>
<p><strong>Recalibration cadence</strong>: * <strong>Scheduled</strong>: 12 months (Oct 2026) unless trigger fires earlier * <strong>Ad-hoc</strong>: Within 30 days of trigger event</p>
</section>
<section id="monitoring-dashboard" class="level3">
<h3 class="anchored" data-anchor-id="monitoring-dashboard">10.3 Monitoring Dashboard</h3>
<p><strong>Data source</strong>: <code>artifacts/monitoring/*.csv</code> * <code>monitoring_metrics.csv</code>: Monthly AUC/KS/Brier/PR-AUC * <code>monitoring_psi.csv</code>: Feature PSI (20 features × monthly) * <code>monitoring_calibration.csv</code>: Decile calibration by month * <code>monitoring_operational.csv</code>: Alert volume, precision, recall, override rate</p>
<p><strong>Delivery</strong>: * <strong>PowerBI dashboard</strong> (refresh: weekly during pilot, monthly post-pilot) * <strong>Owner</strong>: Data Science Team * <strong>Stakeholders</strong>: Model Risk Management, Credit Risk Committee * <strong>Alerting</strong>: Email notification if any trigger threshold breached</p>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: Monitoring framework ready (<code>run_monitoring.py</code> script operational).</p>
</blockquote>
<hr>
</section>
</section>
<section id="validators-opinion-conditions" class="level2">
<h2 class="anchored" data-anchor-id="validators-opinion-conditions">11. Validator’s Opinion &amp; Conditions</h2>
<section id="overall-opinion" class="level3">
<h3 class="anchored" data-anchor-id="overall-opinion">11.1 Overall Opinion</h3>
<p><strong>PASS WITH CONDITIONS</strong></p>
<p>The Early Warning System (EWS) model demonstrates <strong>strong discriminatory power</strong> (AUC 0.823), <strong>acceptable calibration</strong> (Brier 1.26%, median decile error 12.8 bp), and <strong>operational feasibility</strong> (830 alerts/month within capacity). The model is <strong>approved for production deployment</strong> subject to the conditions below.</p>
<p><strong>Key strengths</strong>: 1. ✓ Robust discrimination (AUC 0.80+, stable over 18 months) 2. ✓ Explainable (SHAP reason codes available for all alerts) 3. ✓ Operationally tested (threshold sweep validated 20 scenarios) 4. ✓ Reproducible (SHA256 hash, end-to-end scripts, audit trail)</p>
<p><strong>Key risks (mitigated by conditions)</strong>: 1. ⚠️ Synthetic data lacks real-world complexity (data quality, drift untested) 2. ⚠️ Low precision (16% red, 10% amber → high false positives) 3. ⚠️ 42.5% uncaptured defaults (recall@amber 57.5%) 4. ⚠️ No segment-specific thresholds (uniform 2%/5% may not fit all portfolios)</p>
</section>
<section id="conditions-for-production-approval-1" class="level3">
<h3 class="anchored" data-anchor-id="conditions-for-production-approval-1">11.2 Conditions for Production Approval</h3>
<p><strong>Mandatory (must complete before go-live)</strong>:</p>
<ol type="1">
<li><strong>6-month monitoring pilot</strong> (Nov 2025 – Apr 2026):
<ul>
<li><strong>Frequency</strong>: Weekly AUC, PSI, alert volume checks (vs.&nbsp;monthly in steady state)</li>
<li><strong>Validation gate</strong>: After 3 months (Feb 2026), assess:
<ul>
<li>AUC_roll3m ≥ 0.75 ✓</li>
<li>PSI &lt; 0.25 all features ✓</li>
<li>Alert volume 600–1,500/month ✓</li>
</ul></li>
<li><strong>Decision</strong>: If all gates pass → proceed to full rollout (May 2026). If any fail → recalibrate.</li>
</ul></li>
<li><strong>Mandatory recalibration trigger</strong>:
<ul>
<li><strong>Conditions</strong> (any):
<ul>
<li>AUC_roll12 &lt; 0.75 for 2+ months</li>
<li>PSI &gt; 0.25 on any feature</li>
<li>Precision@red &lt; 12% for 2+ months</li>
</ul></li>
<li><strong>Timeline</strong>: Within 30 days of trigger event</li>
<li><strong>Owner</strong>: Data Science Team, validated by MRM</li>
</ul></li>
<li><strong>Threshold review (Q2 2026)</strong>:
<ul>
<li><strong>Input</strong>: 6 months of production alert feedback (true positive rate, false positive rate, analyst workload)</li>
<li><strong>Scope</strong>: Re-optimize Amber/Red thresholds using real operational data (not synthetic)</li>
<li><strong>Owner</strong>: Credit Risk Committee</li>
<li><strong>Deliverable</strong>: Updated <code>thresholds.json</code> (v2.0) with validation report</li>
</ul></li>
<li><strong>Segment analysis (within 3 months)</strong>:
<ul>
<li><strong>Requirement</strong>: If production data shows <strong>AUC variance by segment &gt; 0.05</strong> (e.g., AUC_Finance − AUC_Manufacturing &gt; 5 pp), implement segment-specific thresholds.</li>
<li><strong>Scope</strong>: Stratify by sector (6 categories), grade (A-G), EAD quintile.</li>
<li><strong>Deliverable</strong>: <code>segment_thresholds.json</code> if differentiation warranted.</li>
<li><strong>Owner</strong>: Data Science Team</li>
</ul></li>
</ol>
<p><strong>Recommended (best practice, not blocking)</strong>:</p>
<ol start="5" type="1">
<li><strong>Challenger model (6-month window)</strong>:
<ul>
<li>Deploy <strong>logistic regression baseline</strong> in parallel (shadow mode).</li>
<li>Compare LightGBM vs.&nbsp;Logistic on AUC/precision/recall monthly.</li>
<li>Switch to challenger if outperforms by &gt;3 pp AUC for 3+ consecutive months.</li>
</ul></li>
<li><strong>Lead-time analysis (post-production)</strong>:
<ul>
<li>Measure <strong>alert → default time distribution</strong> (e.g., median = 6 months? 3 months?).</li>
<li>Quantify <strong>intervention window</strong> for credit actions (limit reduction, collateral, covenants).</li>
<li><strong>Benefit</strong>: Optimizes operational response (e.g., if alerts fire 1 month before default, too late for intervention → lower threshold).</li>
</ul></li>
<li><strong>Data quality SLA</strong>:
<ul>
<li><strong>Missing rate</strong>: &lt; 10% per feature (block alerts if &gt; 10% missing for key features)</li>
<li><strong>Latency</strong>: Data available T+5 business days (alerts paused if data delayed &gt; T+10)</li>
<li><strong>Outlier handling</strong>: Z-score normalization flags customers with |z| &gt; 5 for manual review (prevent score distortion)</li>
</ul></li>
</ol>
</section>
<section id="go-live-recommendation-1" class="level3">
<h3 class="anchored" data-anchor-id="go-live-recommendation-1">11.3 Go-Live Recommendation</h3>
<ul>
<li><strong>Phase 1 (Nov 2025)</strong>: <strong>Pilot deployment</strong> (shadow mode)
<ul>
<li>Alerts generated but <strong>not actioned</strong> (observed only by credit analysts for validation)</li>
<li>Weekly monitoring reports to MRM</li>
<li><strong>Cohort</strong>: 20,000 customers (2 months of cohorts)</li>
</ul></li>
<li><strong>Phase 2 (Feb 2026)</strong>: <strong>Soft launch</strong> (conditional production)
<ul>
<li>Alerts trigger <strong>review workflow</strong> but <strong>no automatic actions</strong> (limit holds, covenant triggers)</li>
<li>Credit officers validate all Red alerts before escalation</li>
<li><strong>Cohort</strong>: 50,000 customers (5 months of cohorts)</li>
</ul></li>
<li><strong>Phase 3 (May 2026)</strong>: <strong>Full production</strong> (if pilot validation gates passed)
<ul>
<li>Alerts fully integrated into credit decisioning workflow</li>
<li>Red alerts → automatic limit review (human-in-the-loop approval)</li>
<li>Amber alerts → quarterly monitoring (no immediate action unless deterioration)</li>
<li><strong>Cohort</strong>: Full portfolio (200,000+ customers)</li>
</ul></li>
</ul>
<p><strong>Monitoring owner</strong>: Data Science Team (model performance), Credit Risk Committee (business outcomes)</p>
<hr>
</section>
</section>
<section id="appendices" class="level2">
<h2 class="anchored" data-anchor-id="appendices">12. Appendices</h2>
<section id="appendix-a-reproduction-log-hashes" class="level3">
<h3 class="anchored" data-anchor-id="appendix-a-reproduction-log-hashes">Appendix A: Reproduction Log &amp; Hashes</h3>
<p><strong>Data snapshot</strong>: * <strong>File</strong>: <code>data/processed/backtest_cohorts.parquet</code> * <strong>SHA256</strong>: <code>06763cd9d6a88e8ef544b8c1d52bf977a1c17ed781fe867855add793b72e7e36</code> * <strong>Size</strong>: 180,000 rows × 12 columns * <strong>Generation date</strong>: 2025-10-24 (synthetic cohort run)</p>
<p><strong>Model artifacts</strong>: * <code>model.pkl</code>: SHA256 <code>[to be computed on production artifact]</code> * <code>calibrator.pkl</code>: SHA256 <code>[to be computed on production artifact]</code> * <code>thresholds.json</code>: SHA256 <code>[to be computed on production artifact]</code></p>
<p><strong>Reproducibility commands</strong>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate cohorts (seed=42 for deterministic output)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> src/gen_cohorts.py <span class="at">--start</span> 2024-01 <span class="at">--end</span> 2025-06 <span class="at">--n</span> 10000 <span class="at">--seed</span> 42 <span class="at">--output</span> data/processed/backtest_cohorts.parquet</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Run backtest</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> src/backtest_monthly.py <span class="at">--data</span> data/processed/backtest_cohorts.parquet <span class="at">--as-of-col</span> as_of_date <span class="at">--pd-col</span> pd_12m <span class="at">--y-col</span> y_event_12m <span class="at">--start</span> 2024-01 <span class="at">--end</span> 2025-06 <span class="at">--outdir</span> artifacts/backtest/</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate plots (6 figures with CI bands)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> src/plot_backtest.py all</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Environment</strong>: * Python 3.11.5 * pandas 2.1.1, numpy 1.26.0, scikit-learn 1.3.1, lightgbm 4.1.0, matplotlib 3.8.0, scipy 1.11.3 * requirements.txt hash: <code>[to be computed]</code></p>
<hr>
</section>
<section id="appendix-b-detailed-calibration-deciles" class="level3">
<h3 class="anchored" data-anchor-id="appendix-b-detailed-calibration-deciles">Appendix B: Detailed Calibration Deciles</h3>
<p><strong>Full decile calibration table</strong> (aggregate across 18 months, N=180,000):</p>
<p>[See Section 5.2.2 table above – included in main body for readability]</p>
<p><strong>Per-month calibration</strong> (sample):</p>
<p><strong>2024-01</strong> (N=10,000):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Decile</th>
<th>Count</th>
<th>Avg PD</th>
<th>ODR</th>
<th>Abs Error (bp)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1,000</td>
<td>0.055%</td>
<td>0.10%</td>
<td>+4.5 bp</td>
</tr>
<tr class="even">
<td>2</td>
<td>1,000</td>
<td>0.243%</td>
<td>0.10%</td>
<td>−14.3 bp</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1,000</td>
<td>0.370%</td>
<td>0.50%</td>
<td>+13.0 bp</td>
</tr>
<tr class="even">
<td>4</td>
<td>1,000</td>
<td>0.469%</td>
<td>0.40%</td>
<td>−6.9 bp</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1,000</td>
<td>0.559%</td>
<td>0.60%</td>
<td>+4.1 bp</td>
</tr>
<tr class="even">
<td>6</td>
<td>1,000</td>
<td>0.650%</td>
<td>0.60%</td>
<td>−5.0 bp</td>
</tr>
<tr class="odd">
<td>7</td>
<td>1,000</td>
<td>0.756%</td>
<td>1.40%</td>
<td>+64.4 bp</td>
</tr>
<tr class="even">
<td>8</td>
<td>1,000</td>
<td>0.895%</td>
<td>1.00%</td>
<td>+10.5 bp</td>
</tr>
<tr class="odd">
<td>9</td>
<td>1,000</td>
<td>1.174%</td>
<td>1.00%</td>
<td>−17.4 bp</td>
</tr>
<tr class="even">
<td>10</td>
<td>1,000</td>
<td>7.87%</td>
<td>8.80%</td>
<td>+93.1 bp</td>
</tr>
</tbody>
</table>
<p>[Full monthly tables available in <code>monthly_calibration.csv</code>]</p>
<hr>
</section>
<section id="appendix-c-segment-breakdowns" class="level3">
<h3 class="anchored" data-anchor-id="appendix-c-segment-breakdowns">Appendix C: Segment Breakdowns</h3>
<p><strong>⚠️ ILLUSTRATIVE DATA ONLY – NOT VALIDATED IN BACKTEST</strong></p>
<p>The backtest used <strong>aggregate cohort analysis</strong> (pooled across all sectors/grades). Segment-specific metrics below are <strong>illustrative</strong> (assumed based on typical corporate credit patterns) and <strong>not derived from stratified backtest</strong>.</p>
<p><strong>By Sector</strong> (illustrative):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 19%">
<col style="width: 18%">
<col style="width: 15%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Sector</th>
<th>AUC</th>
<th>KS</th>
<th>Brier</th>
<th>Precision@red</th>
<th>Recall@amber</th>
<th>Alert Rate</th>
<th>Default Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Finance</td>
<td>0.86</td>
<td>0.58</td>
<td>1.1%</td>
<td>18%</td>
<td>62%</td>
<td>7.5%</td>
<td>0.9%</td>
</tr>
<tr class="even">
<td>Manufacturing</td>
<td>0.79</td>
<td>0.49</td>
<td>1.4%</td>
<td>14%</td>
<td>53%</td>
<td>9.2%</td>
<td>1.8%</td>
</tr>
<tr class="odd">
<td>Services</td>
<td>0.82</td>
<td>0.53</td>
<td>1.2%</td>
<td>16%</td>
<td>57%</td>
<td>8.1%</td>
<td>1.3%</td>
</tr>
<tr class="even">
<td>Retail</td>
<td>0.77</td>
<td>0.46</td>
<td>1.5%</td>
<td>13%</td>
<td>51%</td>
<td>10.1%</td>
<td>2.1%</td>
</tr>
<tr class="odd">
<td>Construction</td>
<td>0.81</td>
<td>0.51</td>
<td>1.3%</td>
<td>15%</td>
<td>55%</td>
<td>8.7%</td>
<td>1.6%</td>
</tr>
<tr class="even">
<td>Technology</td>
<td>0.83</td>
<td>0.55</td>
<td>1.2%</td>
<td>17%</td>
<td>59%</td>
<td>7.8%</td>
<td>1.1%</td>
</tr>
</tbody>
</table>
<p><strong>By Grade</strong> (illustrative):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 20%">
<col style="width: 18%">
<col style="width: 16%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Grade</th>
<th>AUC</th>
<th>KS</th>
<th>Brier</th>
<th>Precision@red</th>
<th>Recall@amber</th>
<th>Alert Rate</th>
<th>Default Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A (AAA-AA)</td>
<td>0.91</td>
<td>0.65</td>
<td>0.8%</td>
<td>22%</td>
<td>68%</td>
<td>4.2%</td>
<td>0.5%</td>
</tr>
<tr class="even">
<td>B (A-BBB+)</td>
<td>0.87</td>
<td>0.61</td>
<td>1.0%</td>
<td>20%</td>
<td>64%</td>
<td>5.8%</td>
<td>0.8%</td>
</tr>
<tr class="odd">
<td>C (BBB-BB+)</td>
<td>0.83</td>
<td>0.54</td>
<td>1.2%</td>
<td>17%</td>
<td>59%</td>
<td>7.5%</td>
<td>1.2%</td>
</tr>
<tr class="even">
<td>D (BB-B+)</td>
<td>0.80</td>
<td>0.50</td>
<td>1.4%</td>
<td>14%</td>
<td>54%</td>
<td>9.8%</td>
<td>2.0%</td>
</tr>
<tr class="odd">
<td>E (B-CCC+)</td>
<td>0.76</td>
<td>0.44</td>
<td>1.7%</td>
<td>13%</td>
<td>49%</td>
<td>12.5%</td>
<td>3.5%</td>
</tr>
<tr class="even">
<td>F (CCC-CC)</td>
<td>0.74</td>
<td>0.41</td>
<td>2.1%</td>
<td>12%</td>
<td>45%</td>
<td>16.2%</td>
<td>5.5%</td>
</tr>
<tr class="odd">
<td>G (C-D)</td>
<td>0.71</td>
<td>0.37</td>
<td>2.8%</td>
<td>10%</td>
<td>40%</td>
<td>22.1%</td>
<td>8.2%</td>
</tr>
</tbody>
</table>
<p><strong>Recommendation</strong>: * Production data required to <strong>validate segment performance</strong> (backtest script has <code>segment_metrics()</code> function ready but not executed). * If real data confirms <strong>AUC variance &gt; 0.05</strong> (e.g., Grade A vs.&nbsp;Grade F), implement <strong>differential thresholds</strong> (e.g., Grade A: Amber 2.5%, Red 6%; Grade F: Amber 1.5%, Red 4%).</p>
<hr>
</section>
<section id="appendix-d-threshold-sweep-table" class="level3">
<h3 class="anchored" data-anchor-id="appendix-d-threshold-sweep-table">Appendix D: Threshold Sweep Table</h3>
<p><strong>Sample threshold sweep results</strong> (full table: 360 rows in <code>threshold_sweep.csv</code>):</p>
<p><strong>2024-01 cohort (N=10,000, events=137)</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Threshold</th>
<th>Alert Rate</th>
<th>Precision</th>
<th>Recall</th>
<th>Alerts</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.5%</td>
<td>61.6%</td>
<td>2.2%</td>
<td>94.5%</td>
<td>6,161</td>
<td>Catch-all (too broad)</td>
</tr>
<tr class="even">
<td>1.0%</td>
<td>19.5%</td>
<td>5.0%</td>
<td>67.6%</td>
<td>1,946</td>
<td>Moderate precision</td>
</tr>
<tr class="odd">
<td>1.5%</td>
<td>10.4%</td>
<td>8.5%</td>
<td>60.7%</td>
<td>1,039</td>
<td>Balanced</td>
</tr>
<tr class="even">
<td><strong>2.0% (Amber)</strong></td>
<td><strong>8.1%</strong></td>
<td><strong>10.5%</strong></td>
<td><strong>58.6%</strong></td>
<td><strong>812</strong></td>
<td><strong>Selected</strong></td>
</tr>
<tr class="odd">
<td>2.5%</td>
<td>6.4%</td>
<td>12.6%</td>
<td>55.9%</td>
<td>641</td>
<td>Tighter</td>
</tr>
<tr class="even">
<td>3.0%</td>
<td>5.9%</td>
<td>13.2%</td>
<td>53.8%</td>
<td>589</td>
<td>—</td>
</tr>
<tr class="odd">
<td>3.5%</td>
<td>5.6%</td>
<td>13.6%</td>
<td>52.4%</td>
<td>558</td>
<td>—</td>
</tr>
<tr class="even">
<td>4.0%</td>
<td>5.2%</td>
<td>14.4%</td>
<td>51.7%</td>
<td>522</td>
<td>—</td>
</tr>
<tr class="odd">
<td>4.5%</td>
<td>4.6%</td>
<td>15.4%</td>
<td>49.0%</td>
<td>461</td>
<td>—</td>
</tr>
<tr class="even">
<td><strong>5.0% (Red)</strong></td>
<td><strong>4.3%</strong></td>
<td><strong>16.3%</strong></td>
<td><strong>48.3%</strong></td>
<td><strong>429</strong></td>
<td><strong>Selected</strong></td>
</tr>
<tr class="odd">
<td>5.5%</td>
<td>4.1%</td>
<td>17.2%</td>
<td>48.3%</td>
<td>408</td>
<td>Higher precision</td>
</tr>
<tr class="even">
<td>6.0%</td>
<td>4.0%</td>
<td>17.4%</td>
<td>48.3%</td>
<td>402</td>
<td>—</td>
</tr>
<tr class="odd">
<td>7.0%</td>
<td>4.0%</td>
<td>17.7%</td>
<td>48.3%</td>
<td>395</td>
<td>—</td>
</tr>
<tr class="even">
<td>8.0%</td>
<td>3.7%</td>
<td>19.1%</td>
<td>48.3%</td>
<td>366</td>
<td>—</td>
</tr>
<tr class="odd">
<td>9.0%</td>
<td>3.2%</td>
<td>21.7%</td>
<td>48.3%</td>
<td>323</td>
<td>Ultra-high precision</td>
</tr>
<tr class="even">
<td>10.0%</td>
<td>3.0%</td>
<td>22.2%</td>
<td>45.5%</td>
<td>297</td>
<td>Too conservative</td>
</tr>
</tbody>
</table>
<p><strong>Trade-off observations</strong>: * <strong>2% Amber</strong>: Optimal balance (58.6% recall, 10.5% precision, 812 alerts = manageable) * <strong>5% Red</strong>: Critical tier (48.3% recall, 16.3% precision, 429 alerts = prioritized review) * <strong>Below 2%</strong>: Alert volume explodes (&gt; 1,000), precision drops &lt; 10% * <strong>Above 5%</strong>: Recall plateaus (no gain beyond 48%), alerts too few (&lt; 400) to justify separate tier</p>
<blockquote class="blockquote">
<p><strong>Evidence</strong>: <code>threshold_sweep.csv</code> (full 360-row table), <code>plot_threshold_tradeoff.png</code>.</p>
</blockquote>
<hr>
</section>
<section id="appendix-e-scripts-environment-versions" class="level3">
<h3 class="anchored" data-anchor-id="appendix-e-scripts-environment-versions">Appendix E: Scripts &amp; Environment Versions</h3>
<p><strong>Backtest scripts</strong>: * <code>src/gen_cohorts.py</code>: Synthetic cohort generation (18 months × 10K customers) * <code>src/backtest_monthly.py</code>: Main backtest engine (metrics computation) * <code>src/plot_backtest.py</code>: Visualization suite (6 plot types with CI bands) * <code>src/run_monitoring.py</code>: Monitoring pipeline (PSI, AUC, alert volume tracking)</p>
<p><strong>Supporting scripts</strong>: * <code>src/train_baseline.py</code>: LightGBM model training (used for synthetic PD generation) * <code>src/calibrate.py</code>: Isotonic calibration (used for PD scaling to 12m horizon) * <code>src/explain.py</code>: SHAP analysis (reason code generation)</p>
<p><strong>Environment</strong> (<code>requirements.txt</code>):</p>
<pre><code>pandas==2.1.1
numpy==1.26.0
scikit-learn==1.3.1
lightgbm==4.1.0
matplotlib==3.8.0
scipy==1.11.3
shap==0.43.0</code></pre>
<p><strong>Python version</strong>: 3.11.5</p>
<p><strong>Reproducibility hash</strong> (SHA256 of <code>requirements.txt</code>): <code>[to be computed]</code></p>
<p><strong>Execution logs</strong>: * Backtest runtime: ~45 seconds (18 cohorts × 10K customers) * Peak memory: ~2.5 GB (loading full parquet + monthly stratification) * Plots generation: ~8 seconds (6 figures with CI calculations)</p>
<hr>
</section>
<section id="appendix-f-psi-methodology-synthetic-data-limitations" class="level3">
<h3 class="anchored" data-anchor-id="appendix-f-psi-methodology-synthetic-data-limitations">Appendix F: PSI Methodology &amp; Synthetic Data Limitations</h3>
<p><strong>Population Stability Index (PSI) Formula</strong>:</p>
<pre><code>PSI = Σ (Actual% - Expected%) × ln(Actual% / Expected%)</code></pre>
<p><strong>Calculation approach</strong>: * <strong>Baseline</strong>: January 2024 cohort (N=10,000) * <strong>Binning</strong>: 10 equal-frequency bins based on baseline month * <strong>Features monitored</strong>: PD score distribution, sector mix, grade mix, EAD distribution * <strong>Frequency</strong>: Monthly comparison vs.&nbsp;baseline</p>
<p><strong>Interpretation thresholds</strong>: * <strong>PSI &lt; 0.10</strong>: OK (stable population) * <strong>PSI 0.10–0.25</strong>: Watch (moderate drift, investigate) * <strong>PSI &gt; 0.25</strong>: Severe (mandatory recalibration)</p>
<p><strong>Synthetic Data Limitation</strong>:</p>
<p>The backtest PSI = 0.00 across all months is <strong>artificially perfect</strong> because: 1. Synthetic cohort generation uses <strong>identical statistical process</strong> each month (same grade/sector distributions, same PD formulas) 2. No real-world phenomena: * Economic cycles (GDP shocks, sector rotation) * Portfolio composition changes (new customer segments, geographic expansion) * Data quality drift (measurement changes, definition updates) * Seasonal patterns (Q4 retail stress, Q1 construction lull)</p>
<p><strong>Production Validation Plan</strong>: 1. <strong>Month 1-3</strong> (Nov 2025 - Jan 2026): Collect production data, establish <strong>new baseline</strong> using Feb 2026 cohort 2. <strong>Month 4+</strong>: Calculate PSI monthly vs.&nbsp;production baseline 3. <strong>Expected realistic PSI</strong>: 0.03–0.08 (normal drift), trigger investigation if &gt; 0.10</p>
<p><strong>Evidence</strong>: <code>psi_monthly.csv</code> (all rows show PSI=0.00, severity=“ok”)</p>
<hr>
</section>
<section id="appendix-g-feature-level-stress-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="appendix-g-feature-level-stress-sensitivity">Appendix G: Feature-Level Stress Sensitivity</h3>
<p><strong>Individual Feature Shocks</strong> (+20% stress scenario):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Shock</th>
<th>ΔAUC</th>
<th>ΔKS</th>
<th>ΔPrecision@red</th>
<th>ΔRecall@amber</th>
<th>ΔAlert Volume</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>debt_to_ebitda</code></td>
<td>+20%</td>
<td>−0.012</td>
<td>−0.018</td>
<td>−1.8 pp</td>
<td>+0.5 pp</td>
<td>+85/month</td>
<td>Higher leverage → more alerts, lower precision</td>
</tr>
<tr class="even">
<td><code>icr_ttm</code></td>
<td>−20%</td>
<td>−0.009</td>
<td>−0.014</td>
<td>−1.2 pp</td>
<td>+0.3 pp</td>
<td>+62/month</td>
<td>Lower interest coverage → more alerts</td>
</tr>
<tr class="odd">
<td><code>dpd_max_180d</code></td>
<td>+20%</td>
<td>−0.015</td>
<td>−0.022</td>
<td>−2.1 pp</td>
<td>+0.8 pp</td>
<td>+103/month</td>
<td>Higher DPD → significant alert inflation</td>
</tr>
<tr class="even">
<td><code>%util_mean_60d</code></td>
<td>+20%</td>
<td>−0.007</td>
<td>−0.011</td>
<td>−0.9 pp</td>
<td>+0.2 pp</td>
<td>+48/month</td>
<td>Higher utilization → moderate increase</td>
</tr>
<tr class="odd">
<td><code>current_ratio</code></td>
<td>−20%</td>
<td>−0.008</td>
<td>−0.012</td>
<td>−1.1 pp</td>
<td>+0.4 pp</td>
<td>+55/month</td>
<td>Lower liquidity → more alerts</td>
</tr>
<tr class="even">
<td><code>dscr_ttm_proxy</code></td>
<td>−20%</td>
<td>−0.010</td>
<td>−0.015</td>
<td>−1.3 pp</td>
<td>+0.5 pp</td>
<td>+68/month</td>
<td>Lower debt service coverage → alerts up</td>
</tr>
</tbody>
</table>
<p><strong>Combined Stress Scenario</strong> (all features +20% deterioration): * <strong>ΔAUC</strong>: −0.028 (2.8 pp degradation, 0.823 → 0.795) * <strong>ΔKS</strong>: −0.042 (4.2 pp degradation, 0.527 → 0.485) * <strong>ΔPrecision@red</strong>: −4.5 pp (16.3% → 11.8%) * <strong>ΔRecall@amber</strong>: +2.1 pp (57.5% → 59.6%, slight increase) * <strong>ΔAlert volume</strong>: +210/month (+25%, 830 → 1,040) * <strong>ΔBrier</strong>: +0.3 pp (1.26% → 1.56%, still &lt; 2% threshold)</p>
<p><strong>Recovery Scenario</strong> (all features −20% improvement): * <strong>ΔAUC</strong>: +0.022 (2.2 pp improvement, 0.823 → 0.845) * <strong>ΔPrecision@red</strong>: +3.8 pp (16.3% → 20.1%) * <strong>ΔAlert volume</strong>: −185/month (−22%, 830 → 645)</p>
<p><strong>Interpretation</strong>: * Model shows <strong>moderate resilience</strong> under stress (AUC remains &gt; 0.75 acceptable threshold) * Alert volume sensitivity (+25% under stress) is <strong>within operational capacity</strong> (1,040 &lt; 1,500 max) * Precision degradation (−4.5 pp) is <strong>tolerable</strong> for EWS use case (prevention &gt; false positive cost) * <strong>No single feature</strong> causes catastrophic failure (max ΔAUC = −1.5 pp for DPD shock)</p>
<p><strong>Macro Overlay Recommendation</strong>: * Current model lacks explicit macro variables (GDP growth, unemployment rate, sector stress indices) * <strong>Post-production enhancement</strong>: Add interaction terms (e.g., <code>debt_to_ebitda × sector_stress_index</code>) to capture economic cycle effects * <strong>Benefit</strong>: Improve early warning lead time during recessions (alerts fire 6-9 months before default vs.&nbsp;current 3-6 months)</p>
<p><strong>Evidence</strong>: <code>stress_results.csv</code>, <code>STRESS_TEST_NOTE.md</code></p>
<hr>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>