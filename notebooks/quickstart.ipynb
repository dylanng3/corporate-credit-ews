{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e90c09",
   "metadata": {},
   "source": [
    "# EWS Pipeline - Quick Start Guide\n",
    "**Corporate Credit Early Warning System**\n",
    "\n",
    "This notebook provides executable code to run the complete EWS pipeline from data generation to production scoring.\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "- Python 3.13+ with virtual environment activated\n",
    "- All packages installed: `pip install -r requirements.txt`\n",
    "- Project structure in place (see README.md)\n",
    "\n",
    "## ðŸŽ¯ Pipeline Overview\n",
    "\n",
    "```\n",
    "1. Generate Data        â†’ Synthetic training data\n",
    "2. Feature Engineering  â†’ 20+ financial/behavioral features\n",
    "3. Train Model          â†’ LightGBM + SHAP + Calibration\n",
    "4. Generate Scores      â†’ Production scoring output\n",
    "5. Validate Results     â†’ Backtest + Dashboard\n",
    "```\n",
    "\n",
    "**Estimated runtime**: ~2-3 minutes for full pipeline (1000 customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba5df0",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup: Verify Environment\n",
    "\n",
    "First, let's check that all required packages and directories are in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "âœ“ pandas\n",
      "âœ“ numpy\n",
      "âœ“ pandas\n",
      "âœ“ numpy\n",
      "âœ“ lightgbm\n",
      "âœ“ lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\OneDrive\\Dylandocs\\Projects\\corporate-credit-ews\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ shap\n",
      "âœ“ sklearn\n",
      "âœ— src/ - Missing!\n",
      "âœ— data/ - Missing!\n",
      "âœ— artifacts/ - Missing!\n",
      "âœ— notebooks/ - Missing!\n",
      "\n",
      "âš ï¸ Fix missing directories!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to Python path to enable imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Check key packages\n",
    "packages = [\"pandas\", \"numpy\", \"lightgbm\", \"shap\", \"sklearn\"]\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"âœ“ {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"âœ— {pkg} - Run: pip install {pkg}\")\n",
    "\n",
    "# Check project structure\n",
    "required_dirs = [\"src\", \"data\", \"artifacts\", \"notebooks\"]\n",
    "for d in required_dirs:\n",
    "    if Path(d).exists():\n",
    "        print(f\"âœ“ {d}/\")\n",
    "    else:\n",
    "        print(f\"âœ— {d}/ - Missing!\")\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\" if all(Path(d).exists() for d in required_dirs) else \"\\nâš ï¸ Fix missing directories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c4c4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ STEP 1: Generate Synthetic Data\n",
    "\n",
    "Create training data with 1000 corporate customers:\n",
    "- 5 raw data tables (financials, credit, cashflow, covenant, labels)\n",
    "- 12 quarters of financial history\n",
    "- 180 days of behavioral data\n",
    "- Binary default labels (12-month horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Files created in data/raw/:\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 98.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\Users\\\\Admin\\\\OneDrive\\\\Dylandocs\\\\Projects\\\\corporate-credit-ews\\\\notebooks\\\\src\\\\gen_data\\\\gen_input.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import data generation module\n",
    "from gen_data.gen_input import Config, run\n",
    "\n",
    "# Configure data generation\n",
    "config = Config(\n",
    "    random_seed=42,\n",
    "    n_customers=1000,\n",
    "    end_quarter=\"2025-06-30\",\n",
    "    n_quarters=12,\n",
    "    asof_date=\"2025-06-30\",\n",
    "    behavior_days=180,\n",
    "    label_horizon_days=365,\n",
    "    output_dir=\"data/raw\"\n",
    ")\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"ðŸ”„ Generating synthetic corporate data...\")\n",
    "result = run(config)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nðŸ“‚ Files created in data/raw/:\")\n",
    "for f in Path(\"data/raw\").glob(\"*.parquet\"):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  â€¢ {f.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"  Customers: {result['summary']['n_customers']}\")\n",
    "print(f\"  Default rate (12M): {result['summary']['event_rate_h12m']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a1b39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ STEP 2: Feature Engineering\n",
    "\n",
    "Transform raw tables into modeling features:\n",
    "- **Financial ratios**: Debt/EBITDA, ICR, Current Ratio, etc.\n",
    "- **Credit behavior**: Utilization, DPD, breach counts\n",
    "- **Cashflow metrics**: Negative days, volatility\n",
    "- **Covenant tracking**: Breach frequency\n",
    "- **Normalization**: Z-scores by sector & size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import feature engineering module\n",
    "from modeling.feature_engineering import make_features\n",
    "\n",
    "# Build features\n",
    "print(\"ðŸ”§ Engineering features...\")\n",
    "df_features = make_features(\n",
    "    raw_dir=Path(\"data/raw\"),\n",
    "    asof=\"2025-06-30\",\n",
    "    winsor=True,\n",
    "    normalize=True,\n",
    "    out_path=Path(\"data/processed/feature_ews.parquet\")\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Features created: {df_features.shape[0]} customers Ã— {df_features.shape[1]} columns\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_features['event_h12m'].value_counts())\n",
    "print(f\"Default rate: {df_features['event_h12m'].mean():.2%}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Feature columns:\")\n",
    "print(f\"  Normalized features: {len([c for c in df_features.columns if '__zs_' in c])}\")\n",
    "print(f\"  Raw features: {len([c for c in df_features.columns if '__zs_' not in c])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51113c05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ¤– STEP 3: Train Model\n",
    "\n",
    "Train LightGBM classifier with:\n",
    "- **80/20 train-test split** (stratified)\n",
    "- **Platt calibration** for probability adjustment\n",
    "- **SHAP explainability** for feature importance\n",
    "- **Percentile thresholds**: Red = top 5%, Amber = top 10%\n",
    "\n",
    "**Outputs:**\n",
    "- `model_lgbm.pkl` - Trained model bundle\n",
    "- `baseline_metrics.json` - AUC, KS, Brier scores\n",
    "- `scores_calibrated.csv` - All predictions with tiers\n",
    "- `shap_summary.csv` - Feature importance rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import training module\n",
    "from modeling.train_baseline import train_and_calibrate\n",
    "\n",
    "# Train model with calibration\n",
    "print(\"ðŸ¤– Training LightGBM with isotonic calibration...\")\n",
    "result = train_and_calibrate(\n",
    "    df=df_features,\n",
    "    target_col=\"event_h12m\",\n",
    "    test_size=0.2,\n",
    "    seed=42,\n",
    "    red_pct=0.05,     # Top 5% = Red\n",
    "    amber_pct=0.10,   # Next 10% = Amber\n",
    "    outdir=Path(\"artifacts/models\")\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nðŸ“Š Model Performance:\")\n",
    "metrics = result['metrics']\n",
    "print(f\"  AUC:    {metrics['AUC']:.3f} (target > 0.80)\")\n",
    "print(f\"  KS:     {metrics['KS']:.3f}\")\n",
    "print(f\"  Brier:  {metrics['Brier']:.4f} (target < 0.02)\")\n",
    "print(f\"  PR-AUC: {metrics['PR_AUC']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Thresholds (percentile-based):\")\n",
    "thr = result['thresholds']\n",
    "print(f\"  Red tier:   PD â‰¥ {thr['red']:.2%}\")\n",
    "print(f\"  Amber tier: PD â‰¥ {thr['amber']:.2%}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Artifacts saved to: {result['artifacts_dir']}\")\n",
    "print(f\"  â€¢ model_lgbm.pkl\")\n",
    "print(f\"  â€¢ thresholds.json\")\n",
    "print(f\"  â€¢ baseline_metrics.json\")\n",
    "print(f\"  â€¢ shap_summary.csv + .png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad62bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š STEP 4: View Feature Importance\n",
    "\n",
    "Check which features drive the model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 features from SHAP analysis\n",
    "df_shap = pd.read_csv(\"artifacts/models/shap_summary.csv\")\n",
    "print(\"ðŸ” Top 10 Most Important Features:\\n\")\n",
    "print(df_shap.head(10).to_string(index=False))\n",
    "\n",
    "# Show SHAP summary plot if available\n",
    "from IPython.display import Image, display\n",
    "shap_plot = Path(\"artifacts/models/shap_summary.png\")\n",
    "if shap_plot.exists():\n",
    "    print(\"\\nðŸ“ˆ SHAP Summary Plot:\")\n",
    "    display(Image(filename=str(shap_plot), width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dbba2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ STEP 5: Production Scoring\n",
    "\n",
    "Score production portfolio (use generated data or upload your own):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import portfolio creation module\n",
    "from gen_data.gen_portfolio import create_portfolio\n",
    "\n",
    "# Create portfolio from scored data\n",
    "print(\"ðŸŽ¯ Creating production portfolio...\")\n",
    "df_portfolio = create_portfolio(\n",
    "    scores_path=Path(\"data/processed/scores_calibrated.csv\"),\n",
    "    output_path=Path(\"data/processed/portfolio_scored.csv\"),\n",
    "    seed=42,\n",
    "    id_col=\"customer_id\",\n",
    "    sector_col=\"sector_code\",\n",
    "    pd_col=\"prob_calibrated\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Portfolio created: {len(df_portfolio)} customers\")\n",
    "print(f\"   Saved to: data/processed/portfolio_scored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29abcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import scoring module\n",
    "from scoring import run_pipeline\n",
    "\n",
    "# Score the portfolio\n",
    "print(\"ðŸ“Š Scoring production portfolio...\")\n",
    "result = run_pipeline(\n",
    "    features_path=Path(\"data/processed/feature_ews.parquet\"),\n",
    "    model_path=Path(\"artifacts/models/model_lgbm.pkl\"),\n",
    "    thr_path=Path(\"artifacts/models/thresholds.json\"),\n",
    "    asof=\"2025-06-30\",\n",
    "    outdir=Path(\"artifacts/scoring\")\n",
    ")\n",
    "\n",
    "# View scoring results\n",
    "df_scored = pd.read_csv(\"artifacts/scoring/ews_scored_2025-06-30.csv\")\n",
    "print(f\"\\nðŸ“‹ Scored {len(df_scored)} customers\")\n",
    "print(\"\\nTier Distribution:\")\n",
    "print(df_scored['tier'].value_counts())\n",
    "print(\"\\nSample High-Risk Customers:\")\n",
    "print(df_scored[df_scored['tier'] == 'Red'][['customer_id', 'prob_default_12m_calibrated', 'score_ews', 'tier']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaaaaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… STEP 6: Validation & Backtesting\n",
    "\n",
    "Run backtest over 18 months to validate model stability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import cohort generation\n",
    "from gen_data.gen_cohorts import generate_synthetic_cohorts\n",
    "\n",
    "# Generate 18 months of backtest data\n",
    "print(\"ðŸ“… Generating backtest cohorts...\")\n",
    "df_cohorts = generate_synthetic_cohorts(\n",
    "    start=\"2024-01-31\",\n",
    "    end=\"2025-06-30\",\n",
    "    n_customers=10000,\n",
    "    seed=42,\n",
    "    output=\"data/processed/backtest_cohorts.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Backtest cohorts created\")\n",
    "print(f\"   Months: {df_cohorts['as_of_date'].nunique()}\")\n",
    "print(f\"   Total rows: {len(df_cohorts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035acfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import backtest module\n",
    "from backtest.backtest_monthly import run_backtest\n",
    "\n",
    "# Run monthly backtest\n",
    "print(\"ðŸ”„ Running monthly backtest...\")\n",
    "monthly_df, calibration_df = run_backtest(\n",
    "    data_path=Path(\"data/processed/backtest_cohorts.parquet\"),\n",
    "    as_of_col=\"as_of_date\",\n",
    "    pd_col=\"pd_12m\",\n",
    "    y_col=\"y_event_12m\",\n",
    "    start_month=\"2024-01\",\n",
    "    end_month=\"2025-06\",\n",
    "    outdir=Path(\"artifacts/backtest\")\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nðŸ“Š Backtest Results ({len(monthly_df)} months):\\n\")\n",
    "print(f\"AUC:   mean={monthly_df['auc'].mean():.3f}, range={monthly_df['auc'].min():.3f}â€“{monthly_df['auc'].max():.3f}\")\n",
    "print(f\"Brier: mean={monthly_df['brier'].mean():.4f}\")\n",
    "print(f\"Amber Alert Rate: {monthly_df['amber_alert_rate'].mean():.1%}\")\n",
    "print(f\"Amber Precision:  {monthly_df['amber_precision'].mean():.1%}\")\n",
    "print(f\"Amber Recall:     {monthly_df['amber_recall'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import PSI calculation\n",
    "from backtest.calculate_psi import compute_psi_monthly\n",
    "\n",
    "# Calculate PSI\n",
    "print(\"ðŸ“‰ Calculating Population Stability Index...\")\n",
    "df_psi = compute_psi_monthly(\n",
    "    cohorts_path=Path(\"data/processed/backtest_cohorts.parquet\"),\n",
    "    pd_col=\"pd_12m\",\n",
    "    as_of_col=\"as_of_date\",\n",
    "    output_path=Path(\"artifacts/backtest/psi_monthly.csv\")\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“‰ PSI Stability Check:\")\n",
    "print(df_psi.tail(5).to_string(index=False))\n",
    "print(f\"\\nMax PSI: {df_psi['psi'].max():.4f} (threshold: 0.10 = warning, 0.25 = critical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f2aa2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š STEP 7: Generate Validation Dashboard\n",
    "\n",
    "Create visual validation report with 5 plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import plot generation\n",
    "from plot_validation import create_all_plots\n",
    "\n",
    "# Generate validation plots\n",
    "print(\"ðŸ“Š Generating validation plots...\")\n",
    "plots_created = create_all_plots(\n",
    "    monthly_metrics_path=Path(\"artifacts/backtest/monthly_metrics.csv\"),\n",
    "    calibration_path=Path(\"artifacts/backtest/monthly_calibration.csv\"),\n",
    "    output_dir=Path(\"artifacts/validation/plots\")\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Plots generated in artifacts/validation/plots/:\")\n",
    "for plot_name in plots_created:\n",
    "    print(f\"   â€¢ {plot_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba39c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation dashboard\n",
    "from IPython.display import Image, display\n",
    "\n",
    "dashboard = Path(\"artifacts/validation/plots/validation_dashboard.png\")\n",
    "if dashboard.exists():\n",
    "    print(\"ðŸ“Š Validation Dashboard:\")\n",
    "    display(Image(filename=str(dashboard), width=1000))\n",
    "else:\n",
    "    print(\"âš ï¸ Dashboard not found. Run plot_validation.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601b475",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§ª OPTIONAL: Stress Testing\n",
    "\n",
    "Test model under crisis scenarios (recession, sector shock, credit crunch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import stress testing\n",
    "from stress_test import run_stress_test\n",
    "\n",
    "# Run stress tests\n",
    "print(\"ðŸ”¥ Running stress scenarios...\")\n",
    "df_stress = run_stress_test(\n",
    "    portfolio_path=Path(\"data/processed/portfolio_scored.csv\"),\n",
    "    scenarios_path=Path(\"artifacts/stress_testing/stress_scenarios.yaml\"),\n",
    "    output_path=Path(\"artifacts/stress_testing/stress_results.csv\")\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”¥ Stress Test Results:\\n\")\n",
    "print(df_stress[['scenario', 'baseline_red_pct', 'stressed_red_pct', 'migration_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf86c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¦ Output Summary\n",
    "\n",
    "After running this notebook, you'll have:\n",
    "\n",
    "### ðŸ“ Data Files\n",
    "- `data/raw/*.parquet` - Raw input tables (5 files: financials, credit, cashflow, covenant, labels)\n",
    "- `data/processed/feature_ews.parquet` - Modeling features (20+ engineered features)\n",
    "- `data/processed/scores_calibrated.csv` - All scored customers with tier assignments\n",
    "- `data/processed/backtest_cohorts.parquet` - 18 months backtest data\n",
    "\n",
    "### ðŸ¤– Model Artifacts\n",
    "- `artifacts/models/model_lgbm.pkl` - Trained LightGBM model bundle\n",
    "- `artifacts/models/baseline_metrics.json` - Performance metrics (AUC, KS, Brier, PR-AUC)\n",
    "- `artifacts/models/thresholds.json` - Red/Amber cutoffs (percentile-based)\n",
    "- `artifacts/shap/feature_importance.csv` - SHAP feature rankings\n",
    "- `artifacts/shap/top_drivers_per_customer.csv` - Individual explanations\n",
    "\n",
    "### ðŸ“Š Validation Reports\n",
    "- `artifacts/validation/plots/*.png` - 5 validation charts\n",
    "- `artifacts/validation/VALIDATION_REPORT_EN.md` - Full validation documentation\n",
    "- `artifacts/backtest/monthly_metrics.csv` - 18-month performance tracking\n",
    "- `artifacts/backtest/psi_monthly.csv` - Population stability monitoring\n",
    "\n",
    "### ðŸŽ¯ Scoring Outputs\n",
    "- `artifacts/scoring/ews_scored_2025-06-30.csv` - Production scores with tiers\n",
    "- `artifacts/stress_testing/stress_results.csv` - Scenario analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ee9fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ Re-run Individual Steps\n",
    "\n",
    "If you need to re-run specific steps without rerunning the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train model only (if you modified hyperparameters)\n",
    "result = train_and_calibrate(\n",
    "    df=df_features,\n",
    "    target_col=\"event_h12m\",\n",
    "    test_size=0.2,\n",
    "    seed=42,\n",
    "    red_pct=0.05,\n",
    "    amber_pct=0.10,\n",
    "    outdir=Path(\"artifacts/models\")\n",
    ")\n",
    "\n",
    "# Re-score portfolio only (if you have new customer data)\n",
    "result = run_pipeline(\n",
    "    features_path=Path(\"data/processed/feature_ews.parquet\"),\n",
    "    model_path=Path(\"artifacts/models/model_lgbm.pkl\"),\n",
    "    thr_path=Path(\"artifacts/models/thresholds.json\"),\n",
    "    asof=\"2025-06-30\",\n",
    "    outdir=Path(\"artifacts/scoring\")\n",
    ")\n",
    "\n",
    "# Regenerate plots only\n",
    "plots = create_all_plots(\n",
    "    monthly_metrics_path=Path(\"artifacts/backtest/monthly_metrics.csv\"),\n",
    "    calibration_path=Path(\"artifacts/backtest/monthly_calibration.csv\"),\n",
    "    output_dir=Path(\"artifacts/validation/plots\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40a7ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Next Steps\n",
    "\n",
    "1. **Review validation report**: Open `artifacts/validation/VALIDATION_REPORT_EN.md` in your browser or editor\n",
    "2. **Inspect SHAP drivers**: Check `artifacts/shap/top_drivers_per_customer.csv` to understand individual predictions\n",
    "3. **Analyze backtest stability**: Review `artifacts/backtest/monthly_metrics.csv` for performance trends\n",
    "4. **Customize thresholds**: Edit Red/Amber cutoffs in `artifacts/models/thresholds.json` if needed\n",
    "5. **Deploy to production**: Use `src/scoring.py` with your real customer data\n",
    "\n",
    "**For detailed technical documentation**, see: `reports/PIPELINE_DOCUMENTATION.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c6e6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ†˜ Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. Import errors**\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**2. File not found errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed103111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data files exist\n",
    "import os\n",
    "print(\"ðŸ“‚ data/raw/:\")\n",
    "for f in os.listdir(\"data/raw\"):\n",
    "    print(f\"   â€¢ {f}\")\n",
    "\n",
    "print(\"\\nðŸ“‚ data/processed/:\")\n",
    "for f in os.listdir(\"data/processed\"):\n",
    "    print(f\"   â€¢ {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6603b5b",
   "metadata": {},
   "source": [
    "**3. Model training fails**\n",
    "- Check default rate > 1% (need sufficient positive examples)\n",
    "- Verify features have no missing values\n",
    "- Reduce dataset size if memory issues: use `--n 500` instead of `--n 1000`\n",
    "\n",
    "**4. Plots not displaying**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate all plots\n",
    "plots = create_all_plots(\n",
    "    monthly_metrics_path=Path(\"artifacts/backtest/monthly_metrics.csv\"),\n",
    "    calibration_path=Path(\"artifacts/backtest/monthly_calibration.csv\"),\n",
    "    output_dir=Path(\"artifacts/validation/plots\")\n",
    ")\n",
    "\n",
    "# Check if files were created\n",
    "print(\"ðŸ“Š Generated plots:\")\n",
    "for plot in plots:\n",
    "    print(f\"   â€¢ {plot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffc0ec",
   "metadata": {},
   "source": [
    "### Performance Tips\n",
    "\n",
    "- **Quick testing**: Use `--n 500` for faster experimentation (30 seconds total)\n",
    "- **Full production run**: Use `--n 10000` for realistic evaluation (~5 minutes)\n",
    "- **Parallel processing**: Set `n_jobs=-1` in train_baseline.py to use all CPU cores\n",
    "- **Memory optimization**: If you get MemoryError, reduce `--n` parameter or close other applications\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ž Support\n",
    "\n",
    "- **Full Documentation**: `reports/PIPELINE_DOCUMENTATION.md`\n",
    "- **Repository**: corporate-credit-ews\n",
    "- **Validation Report**: `artifacts/validation/VALIDATION_REPORT_EN.md`\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **You're all set!** Run the cells above from top to bottom (Shift+Enter) to execute the complete EWS pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
