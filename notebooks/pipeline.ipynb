{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86858a9",
   "metadata": {},
   "source": [
    "# Corporate Credit Early Warning System (EWS)\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "H·ªá th·ªëng c·∫£nh b√°o s·ªõm r·ªßi ro t√≠n d·ª•ng doanh nghi·ªáp (Corporate Credit EWS) ƒë∆∞·ª£c x√¢y d·ª±ng theo chu·∫©n Basel, s·ª≠ d·ª•ng Machine Learning ƒë·ªÉ d·ª± ƒëo√°n x√°c su·∫•t v·ª° n·ª£ (PD - Probability of Default) trong v√≤ng 12 th√°ng t·ªõi.\n",
    "\n",
    "**M·ª•c ti√™u ch√≠nh:**\n",
    "- D·ª± ƒëo√°n kh·∫£ nƒÉng v·ª° n·ª£ c·ªßa kh√°ch h√†ng doanh nghi·ªáp (event horizon: 12 th√°ng)\n",
    "- Ph√¢n lo·∫°i r·ªßi ro th√†nh 3 tiers: **Green** (an to√†n), **Amber** (c·∫£nh b√°o), **Red** (nguy hi·ªÉm)\n",
    "- ƒê∆∞a ra khuy·∫øn ngh·ªã h√†nh ƒë·ªông c·ª• th·ªÉ cho Risk Management team\n",
    "\n",
    "**Tech Stack:**\n",
    "- Python 3.13\n",
    "- LightGBM (classification model)\n",
    "- SHAP (model explainability)\n",
    "- Sklearn (calibration, metrics)\n",
    "- Pandas/Numpy (data processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f0cf4",
   "metadata": {},
   "source": [
    "## Pipeline Architecture\n",
    "\n",
    "D·ª± √°n ƒë∆∞·ª£c t·ªï ch·ª©c theo **end-to-end ML pipeline** v·ªõi 7 b∆∞·ªõc ch√≠nh:\n",
    "\n",
    "```\n",
    "1. Data Generation (generate_data.py)\n",
    "   ‚Üì\n",
    "2. Feature Engineering (feature_engineering.py)\n",
    "   ‚Üì\n",
    "3. Model Training + Calibration (train_baseline.py)\n",
    "   ‚Üì\n",
    "4. Model Explainability (explain.py)\n",
    "   ‚Üì\n",
    "5. [Optional] Make Raw Scores (make_scores_raw.py)\n",
    "   ‚Üì\n",
    "6. [Optional] Re-calibration (calibrate.py)\n",
    "   ‚Üì\n",
    "7. Production Scoring (scoring.py)\n",
    "```\n",
    "\n",
    "### Data Flow\n",
    "- **Raw Data** ‚Üí `data/raw/` (fin_quarterly, credit_daily, cashflow_daily, covenant, labels)\n",
    "- **Features** ‚Üí `data/processed/` (feature_ews.parquet)\n",
    "- **Models** ‚Üí `artifacts/models/` (model_lgbm.pkl, SHAP artifacts)\n",
    "- **Scores** ‚Üí `artifacts/scoring/` (ews_scored_YYYY-MM-DD.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fd11a",
   "metadata": {},
   "source": [
    "## Step 1: Data Generation\n",
    "\n",
    "Module: `src/generate_data.py`\n",
    "\n",
    "### Overview\n",
    "\n",
    "T·∫°o d·ªØ li·ªáu synthetic (gi·∫£ l·∫≠p) ƒë·ªÉ train v√† test model EWS. D·ªØ li·ªáu ƒë∆∞·ª£c thi·∫øt k·∫ø s√°t v·ªõi th·ª±c t·∫ø nghi·ªáp v·ª• t√≠n d·ª•ng doanh nghi·ªáp v√† tu√¢n th·ªß c√°c nguy√™n t·∫Øc Basel.\n",
    "\n",
    "### Why Synthetic Data?\n",
    "\n",
    "#### L·ª£i √≠ch v√† h·∫°n ch·∫ø\n",
    "L·ª£i √≠ch c·ªßa vi·ªác s·ª≠ d·ª•ng d·ªØ li·ªáu t·ªïng h·ª£p l√† tr√°nh ƒë∆∞·ª£c v·∫•n ƒë·ªÅ b·∫£o m·∫≠t v√† r·ªßi ro tu√¢n th·ªß do s·ª≠ d·ª•ng c√°c d·ªØ li·ªáu n·ªôi b·ªô t·ª´ ng√¢n h√†ng. B√™n c·∫°nh ƒë√≥, ta c√≥ th·ªÉ bi·∫øt ch√≠nh x√°c ground truth d·ªØ li·ªáu ƒë·∫ßu v√†o c·ªßa m√¥ h√¨nh (·ªü ƒë√¢y l√† event_12m), ƒëi·ªÅu m√† v√¥ c√πng quan tr·ªçng khi x√¢y d·ª±ng c√°c thu·∫≠t to√°n h·ªçc m√°y nh∆∞ LightGBM. Ngo√†i ra, ch√∫ng ta gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ scaling b·∫±ng vi·ªác d·ªÖ d√†ng t·∫°o 1K, 10K, 100K d·ªØ li·ªáu kh√°ch h√†ng, ƒëi·ªÅu m√† kh√° kh√≥ khƒÉn v·ªõi nh·ªØng d·ª± √°n c√≥ kinh ph√≠ th·∫•p v√† kh·∫£ nƒÉng ti·∫øp c·∫≠n h·∫°n ch·∫ø ƒë·ªëi v·ªõi d·ªØ li·ªáu th·∫≠t. M·ªôt l·ª£i th·∫ø kh√°c khi s·ª≠ d·ª•ng d·ªØ li·ªáu t·ªïng h·ª£p l√† kh·∫£ nƒÉng th·ª±c hi·ªán c√°c edge cases v√† stress scenarios: nh·ªØng tr∆∞·ªùng h·ª£p d·ªØ li·ªáu n·∫±m ·ªü bi√™n c·ªßa ph√¢n ph·ªëi th√¥ng th∆∞·ªùng, r·∫•t √≠t khi xu·∫•t hi·ªán trong d·ªØ li·ªáu th·ª±c t·∫ø.\n",
    "\n",
    "H·∫°n ch·∫ø l·ªõn nh·∫•t c·ªßa vi·ªác s·ª≠ d·ª•ng d·ªØ li·ªáu t·ªïng h·ª£p l√† s·ª± kh√°c bi·ªát v·ªÅ ph√¢n ph·ªëi (distribution shift) so v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø. N·∫øu d·ªØ li·ªáu t·ªïng h·ª£p kh√¥ng m√¥ ph·ªèng ch√≠nh x√°c m·ªëi quan h·ªá ph·ª©c t·∫°p v√† c√°c s·∫Øc th√°i ·∫©n trong d·ªØ li·ªáu g·ªëc c·ªßa ng√¢n h√†ng, m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n ƒë√≥ c√≥ th·ªÉ ho·∫°t ƒë·ªông k√©m hi·ªáu qu·∫£ ho·∫∑c ƒë∆∞a ra c√°c d·ª± ƒëo√°n sai l·ªách nghi√™m tr·ªçng khi √°p d·ª•ng v√†o m√¥i tr∆∞·ªùng th·ª±c t·∫ø. \n",
    "\n",
    "#### Vai tr√≤\n",
    "D·ªØ li·ªáu t·ªïng h·ª£p c√≥ vai tr√≤ thi·∫øt y·∫øu trong nhi·ªÅu giai ƒëo·∫°n c·ªßa d·ª± √°n khoa h·ªçc d·ªØ li·ªáu. N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ch·ª©ng minh t√≠nh kh·∫£ thi (Proof-of-concept) c·ªßa m√¥ h√¨nh ho·∫∑c gi·∫£i ph√°p tr∆∞·ªõc khi √°p d·ª•ng l√™n d·ªØ li·ªáu th·∫≠t, gi√∫p gi·∫£m thi·ªÉu r·ªßi ro. Ngo√†i ra, d·ªØ li·ªáu n√†y l√† c√¥ng c·ª• l√Ω t∆∞·ªüng ƒë·ªÉ hu·∫•n luy·ªán c√°c ƒë·ªôi ng≈© m·ªõi m√† kh√¥ng c·∫ßn ti·∫øp x√∫c v·ªõi th√¥ng tin nh·∫°y c·∫£m.\n",
    "\n",
    "---\n",
    "\n",
    "### Customer Profile Configuration\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Config:\n",
    "    n_customers: int = 1000\n",
    "    sectors: Tuple[str, ...] = (\n",
    "        \"MFG\",  # Manufacturing (18%)\n",
    "        \"TRA\",  # Trading (12%)\n",
    "        \"CON\",  # Construction (10%)\n",
    "        \"AGR\",  # Agriculture (8%)\n",
    "        \"ENG\",  # Engineering (8%)\n",
    "        \"CHE\",  # Chemicals (10%)\n",
    "        \"RET\",  # Retail (12%)\n",
    "        \"LOG\",  # Logistics (10%)\n",
    "        \"TEL\",  # Telecom (6%)\n",
    "        \"IT\"    # Information Technology (6%)\n",
    "    )\n",
    "    size_buckets: Tuple[str, ...] = (\"SME\", \"Corp\")\n",
    "    size_probs: Tuple[float, ...] = (0.8, 0.2)  # 80% SME, 20% Corp\n",
    "```\n",
    "\n",
    "**Sector Risk Premiums:**\n",
    "M·ªói sector c√≥ risk premium kh√°c nhau (th√™m v√†o debt_mult):\n",
    "- **Low risk:** ENG (0.0), MFG (0.0), TEL (-0.01), IT (-0.02)\n",
    "- **Medium risk:** CHE (0.02), LOG (0.02), RET (0.03), CON (0.03)\n",
    "- **Higher risk:** AGR (0.04), TRA (0.05)\n",
    "\n",
    "**Size Characteristics:**\n",
    "- **SME (Small-Medium Enterprise):**\n",
    "  - Base revenue: lognormal(mean=10.5, œÉ=0.5)\n",
    "  - Debt multiplier: 0.8 + 0.4 = 1.2\n",
    "  - Higher default risk due to less stability\n",
    "  \n",
    "- **Corp (Large Corporate):**\n",
    "  - Base revenue: lognormal(mean=12, œÉ=0.5)  ‚Üí ~2.7x larger\n",
    "  - Debt multiplier: 0.8 + 0.6 = 1.4\n",
    "  - More stable but higher leverage\n",
    "\n",
    "---\n",
    "\n",
    "### Data Tables Generated\n",
    "\n",
    "#### 1. **fin_quarterly.parquet** - B√°o c√°o t√†i ch√≠nh theo qu√Ω\n",
    "\n",
    "**Time Range:** 12 quarters (3 years) history ending at `2025-06-30`\n",
    "\n",
    "**Columns (15 total):**\n",
    "\n",
    "| Column | Description | Generation Logic |\n",
    "|--------|-------------|------------------|\n",
    "| `customer_id` | Unique ID (C0001-C1000) | Sequential |\n",
    "| `fq_date` | Quarter end date | 2022-09-30 ‚Üí 2025-06-30 |\n",
    "| `sector_code` | Industry sector | Random t·ª´ 10 sectors |\n",
    "| `size_bucket` | SME or Corp | 80/20 split |\n",
    "| `revenue` | Quarterly revenue | Growth ~2% QoQ + noise |\n",
    "| `cogs` | Cost of Goods Sold | ~75% of revenue |\n",
    "| `ebitda` | Earnings Before Interest, Tax, D&A | ~15% margin + noise |\n",
    "| `ebit` | Earnings Before Interest & Tax | EBITDA - D&A (proxy 2% revenue) |\n",
    "| `interest_expense` | Quarterly interest | Debt √ó 8% annual / 4 |\n",
    "| `total_debt` | Total outstanding debt | Revenue √ó (0.3 + sector_risk) √ó debt_mult |\n",
    "| `current_assets` | AR + Inventory + Cash | Function of revenue/COGS |\n",
    "| `current_liab` | AP + short-term liabilities | Function of COGS/revenue |\n",
    "| `inventory` | Inventory level | ~12% of COGS |\n",
    "| `ar` | Accounts Receivable | ~18% of revenue (DSO ~66 days) |\n",
    "| `ap` | Accounts Payable | ~20% of COGS (DPO ~73 days) |\n",
    "\n",
    "**Realism Features:**\n",
    "- ‚úÖ **Growth patterns:** QoQ growth ~2% ¬± 5% noise\n",
    "- ‚úÖ **Profitability:** EBITDA margin 15% ¬± 7%\n",
    "- ‚úÖ **Leverage:** Debt/Revenue varies by sector\n",
    "- ‚úÖ **Working capital:** Realistic DSO, DPO, Inventory turnover\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **credit_daily.parquet** - H√†nh vi t√≠n d·ª•ng h√†ng ng√†y\n",
    "\n",
    "**Time Range:** \n",
    "- Observation window: 180 days before `asof_date` (2025-01-02 ‚Üí 2025-06-30)\n",
    "- Label window: 365 days after `asof_date` (2025-07-01 ‚Üí 2026-06-30)\n",
    "- **Total:** 545 days per customer\n",
    "\n",
    "**Columns (7 total):**\n",
    "\n",
    "| Column | Description | Generation Logic |\n",
    "|--------|-------------|------------------|\n",
    "| `customer_id` | Customer ID | - |\n",
    "| `date` | Business date | Daily from start to end |\n",
    "| `limit` | Credit limit | 30% of latest revenue √ó randomness |\n",
    "| `utilized` | Amount used | Limit √ó utilization_rate |\n",
    "| `breach_flag` | 1 if utilized > limit | Binary indicator |\n",
    "| `dpd_days` | Days Past Due | Markov chain: 98.5% stay/improve, 1.5% deteriorate |\n",
    "| `product_type` | OD/TERM/TR_LOAN | 70% Overdraft, 20% Term Loan, 10% Trade Finance |\n",
    "\n",
    "**DPD Markov Process:**\n",
    "```python\n",
    "# Each day:\n",
    "if random() < 0.985:\n",
    "    dpd = max(0, dpd - binomial(1, p=0.3))  # 30% chance gi·∫£m 1 ng√†y\n",
    "else:\n",
    "    dpd += choice([1, 3, 7], p=[0.6, 0.3, 0.1])  # 60% +1, 30% +3, 10% +7\n",
    "```\n",
    "\n",
    "**Utilization Pattern:**\n",
    "```python\n",
    "util_level = beta(2, 2)  # Centered around 0.5\n",
    "seasonal = sin(day_of_year/365 * 2œÄ) * 0.05  # ¬±5% seasonality\n",
    "daily_util = util_level + seasonal + noise(0, 0.05)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **cashflow_daily.parquet** - D√≤ng ti·ªÅn h√†ng ng√†y\n",
    "\n",
    "**Time Range:** Same as credit_daily (545 days)\n",
    "\n",
    "**Columns (4 total):**\n",
    "\n",
    "| Column | Description | Generation Logic |\n",
    "|--------|-------------|------------------|\n",
    "| `customer_id` | Customer ID | - |\n",
    "| `date` | Business date | Daily |\n",
    "| `inflow` | Cash inflow | Daily avg revenue √ó seasonality √ó noise |\n",
    "| `outflow` | Cash outflow | ~90% of inflow √ó noise |\n",
    "\n",
    "**Seasonality Model:**\n",
    "```python\n",
    "daily_mean = annual_revenue / 365 √ó uniform(0.6, 1.1)\n",
    "seasonal_factor = 1 + 0.2 √ó sin(day_of_year/365 √ó 2œÄ)\n",
    "inflow = max(0, normal(daily_mean √ó seasonal_factor, œÉ=daily_mean√ó0.3))\n",
    "outflow = max(0, normal(inflow √ó 0.9, œÉ=daily_mean√ó0.25))\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- Detect sudden revenue drops (inflow_drop_60d)\n",
    "- Monitor burn rate (outflow > inflow)\n",
    "- Identify cashflow volatility\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **covenant.parquet** - Covenant tracking\n",
    "\n",
    "**Time Range:** Daily for observation + label windows\n",
    "\n",
    "**Columns (7 total):**\n",
    "\n",
    "| Column | Description | Threshold | Breach Logic |\n",
    "|--------|-------------|-----------|--------------|\n",
    "| `customer_id` | Customer ID | - | - |\n",
    "| `date` | Business date | - | - |\n",
    "| `icr` | Interest Coverage | ‚â• 1.5 | 1 if < 1.5 |\n",
    "| `dscr` | Debt Service Coverage | ‚â• 1.2 | 1 if < 1.2 |\n",
    "| `leverage` | Debt/EBITDA | ‚â§ 4.0 | 1 if > 4.0 |\n",
    "| `breach_icr` | ICR breach flag | - | Binary |\n",
    "| `breach_dscr` | DSCR breach flag | - | Binary |\n",
    "| `breach_leverage` | Leverage breach flag | - | Binary |\n",
    "\n",
    "**Why Important:**\n",
    "- Covenant breach = Early warning signal\n",
    "- Typical in loan agreements\n",
    "- Triggers renegotiation or penalties\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **labels.parquet** - Target variable\n",
    "\n",
    "**Columns (3 total):**\n",
    "\n",
    "| Column | Description | Logic |\n",
    "|--------|-------------|-------|\n",
    "| `customer_id` | Customer ID | - |\n",
    "| `asof_date` | Snapshot date | 2025-06-30 |\n",
    "| `event_h12m` | Default in 12M | 1 if DPD ‚â• 90 for ‚â• 30 consecutive days |\n",
    "\n",
    "**Label Definition (Basel-compliant):**\n",
    "```python\n",
    "def compute_label(future_dpd_series):\n",
    "    \"\"\"\n",
    "    future_dpd_series: DPD values for 365 days after asof_date\n",
    "    \"\"\"\n",
    "    max_consecutive_90plus = 0\n",
    "    current_streak = 0\n",
    "    \n",
    "    for dpd in future_dpd_series:\n",
    "        if dpd >= 90:\n",
    "            current_streak += 1\n",
    "            max_consecutive_90plus = max(max_consecutive_90plus, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    \n",
    "    return 1 if max_consecutive_90plus >= 30 else 0\n",
    "```\n",
    "\n",
    "**Additional Bumps (increase default probability):**\n",
    "- **High utilization bump:** If util_rate > 90% at asof_date ‚Üí +20% PD\n",
    "- **Covenant breach bump:** If any covenant breached ‚Üí +20% PD\n",
    "\n",
    "**Expected Default Rate:** ~5-10% (typical for corporate portfolio)\n",
    "\n",
    "---\n",
    "\n",
    "### Output Files\n",
    "\n",
    "All tables saved in both **Parquet** (preferred) and **CSV** (fallback):\n",
    "\n",
    "```\n",
    "data/raw/\n",
    "‚îú‚îÄ‚îÄ fin_quarterly.parquet       # ~1000 rows √ó 12 quarters = 12K rows\n",
    "‚îú‚îÄ‚îÄ credit_daily.parquet        # ~1000 customers √ó 545 days = 545K rows\n",
    "‚îú‚îÄ‚îÄ cashflow_daily.parquet      # ~1000 customers √ó 545 days = 545K rows\n",
    "‚îú‚îÄ‚îÄ covenant.parquet            # ~1000 customers √ó 545 days = 545K rows\n",
    "‚îî‚îÄ‚îÄ labels.parquet              # 1000 rows (one per customer)\n",
    "```\n",
    "\n",
    "**File sizes:**\n",
    "- Parquet: ~5-10 MB total (compressed)\n",
    "- CSV: ~30-50 MB total (uncompressed)\n",
    "\n",
    "---\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```bash\n",
    "# Generate with default config (1000 customers)\n",
    "python src/generate_data.py --output-dir data/raw\n",
    "\n",
    "# Generate 5000 customers for stress test\n",
    "python src/generate_data.py --n-customers 5000 --output-dir data/raw_large\n",
    "\n",
    "# Custom end date\n",
    "python src/generate_data.py --end-quarter 2024-12-31 --output-dir data/raw_2024\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quality Checks\n",
    "\n",
    "**After generation, verify:**\n",
    "\n",
    "‚úÖ **Completeness:** All 5 files generated  \n",
    "‚úÖ **Row counts:** Consistent customer_ids across tables  \n",
    "‚úÖ **Date ranges:** Correct observation (180d) + label (365d) windows  \n",
    "‚úÖ **Label distribution:** Default rate 5-10%  \n",
    "‚úÖ **Financial sanity:** Revenue > 0, EBITDA margin reasonable, Debt > 0  \n",
    "‚úÖ **DPD distribution:** Majority < 30, some 30-90, few > 90  \n",
    "\n",
    "```python\n",
    "# Quick checks\n",
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_parquet('data/raw/labels.parquet')\n",
    "print(f\"Default rate: {labels['event_h12m'].mean():.1%}\")  # Should be ~5-10%\n",
    "\n",
    "credit = pd.read_parquet('data/raw/credit_daily.parquet')\n",
    "print(f\"Max DPD: {credit['dpd_days'].max()}\")  # Should see some 90+ days\n",
    "print(f\"Breach rate: {credit['breach_flag'].mean():.1%}\")  # Should be low ~1-5%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac84d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate synthetic data v√† verify k·∫øt qu·∫£\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: GENERATE SYNTHETIC DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Command ƒë·ªÉ generate data\n",
    "print(\"\\nüìù Command to generate data:\")\n",
    "print(\"python src/generate_data.py --n-customers 1000 --output-dir data/raw\")\n",
    "\n",
    "print(\"\\nüìä Expected outputs:\")\n",
    "outputs = {\n",
    "    \"fin_quarterly.parquet\": \"~12,000 rows (1000 customers √ó 12 quarters)\",\n",
    "    \"credit_daily.parquet\": \"~545,000 rows (1000 customers √ó 545 days)\",\n",
    "    \"cashflow_daily.parquet\": \"~545,000 rows (1000 customers √ó 545 days)\",\n",
    "    \"covenant.parquet\": \"~545,000 rows (1000 customers √ó 545 days)\",\n",
    "    \"labels.parquet\": \"1,000 rows (1 row per customer)\"\n",
    "}\n",
    "\n",
    "for file, desc in outputs.items():\n",
    "    print(f\"  ‚úì data/raw/{file:30s} ‚Üí {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION AFTER GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# N·∫øu data ƒë√£ t·ªìn t·∫°i, verify n√≥\n",
    "data_dir = \"../data/raw\"\n",
    "if os.path.exists(data_dir):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "        print(\"\\n‚úÖ Data directory found! Checking files...\")\n",
    "        \n",
    "        # Check labels\n",
    "        labels_path = f\"{data_dir}/labels.parquet\"\n",
    "        if os.path.exists(labels_path):\n",
    "            labels = pd.read_parquet(labels_path)\n",
    "            default_rate = labels['event_h12m'].mean()\n",
    "            print(f\"\\nüìå labels.parquet:\")\n",
    "            print(f\"   - Total customers: {len(labels):,}\")\n",
    "            print(f\"   - Default rate (event_h12m=1): {default_rate:.1%}\")\n",
    "            print(f\"   - Expected: 5-10% ‚úì\" if 0.05 <= default_rate <= 0.15 else \"   - Warning: Outside expected range\")\n",
    "        \n",
    "        # Check credit_daily\n",
    "        credit_path = f\"{data_dir}/credit_daily.parquet\"\n",
    "        if os.path.exists(credit_path):\n",
    "            credit = pd.read_parquet(credit_path)\n",
    "            print(f\"\\nüìå credit_daily.parquet:\")\n",
    "            print(f\"   - Total rows: {len(credit):,}\")\n",
    "            print(f\"   - Date range: {credit['date'].min()} to {credit['date'].max()}\")\n",
    "            print(f\"   - Max DPD: {credit['dpd_days'].max()} days\")\n",
    "            print(f\"   - Breach rate: {credit['breach_flag'].mean():.1%}\")\n",
    "            print(f\"   - Avg utilization: {(credit['utilized']/credit['limit']).mean():.1%}\")\n",
    "        \n",
    "        # Check fin_quarterly\n",
    "        fin_path = f\"{data_dir}/fin_quarterly.parquet\"\n",
    "        if os.path.exists(fin_path):\n",
    "            fin = pd.read_parquet(fin_path)\n",
    "            print(f\"\\nüìå fin_quarterly.parquet:\")\n",
    "            print(f\"   - Total rows: {len(fin):,}\")\n",
    "            print(f\"   - Unique customers: {fin['customer_id'].nunique():,}\")\n",
    "            print(f\"   - Quarters: {fin['fq_date'].nunique()}\")\n",
    "            print(f\"   - Avg EBITDA margin: {(fin['ebitda']/fin['revenue']).mean():.1%}\")\n",
    "            print(f\"   - Avg Debt/EBITDA: {(fin['total_debt']/fin['ebitda']).mean():.1f}x\")\n",
    "        \n",
    "        print(\"\\n‚úÖ All checks passed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not verify data: {e}\")\n",
    "        print(\"   Run the generate_data.py script first to create the data.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Data directory not found: {data_dir}\")\n",
    "    print(\"   Run the following command to generate data:\")\n",
    "    print(\"   python src/generate_data.py --n-customers 1000 --output-dir data/raw\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c495f97",
   "metadata": {},
   "source": [
    "## üîß Step 2: Feature Engineering\n",
    "\n",
    "Module: `src/feature_engineering.py`\n",
    "\n",
    "### Overview\n",
    "\n",
    "Feature Engineering l√† b∆∞·ªõc quan tr·ªçng nh·∫•t trong vi·ªác x√¢y d·ª±ng m√¥ h√¨nh Early Warning System, v√¨ n√≥ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ t·ª´ c√°c b·∫£ng t√†i ch√≠nh, h√†nh vi t√≠n d·ª•ng, v√† d√≤ng ti·ªÅn th√†nh c√°c ƒë·∫∑c tr∆∞ng (features) c√≥ s·ª©c m·∫°nh d·ª± ƒëo√°n cao. Qu√° tr√¨nh n√†y k·∫øt h·ª£p ki·∫øn th·ª©c chuy√™n m√¥n v·ªÅ t√≠n d·ª•ng ng√¢n h√†ng v·ªõi k·ªπ thu·∫≠t ph√¢n t√≠ch d·ªØ li·ªáu, t·∫°o ra t·∫≠p h·ª£p c√°c ch·ªâ s·ªë ph·∫£n √°nh ƒë·∫ßy ƒë·ªß t√¨nh h√¨nh t√†i ch√≠nh v√† r·ªßi ro c·ªßa kh√°ch h√†ng doanh nghi·ªáp.\n",
    "\n",
    "C√°c features ƒë∆∞·ª£c thi·∫øt k·∫ø d·ª±a tr√™n c√°c nguy√™n t·∫Øc Basel v√† th·ª±c ti·ªÖn qu·∫£n l√Ω r·ªßi ro t√≠n d·ª•ng, chia th√†nh 5 nh√≥m ch√≠nh: Financial Ratios, Behavioral Features, Cashflow Features, Covenant Breach Flags, v√† Normalization. M·ªói nh√≥m ph·ª•c v·ª• m·ªôt m·ª•c ƒë√≠ch c·ª• th·ªÉ trong vi·ªác ƒë√°nh gi√° kh·∫£ nƒÉng v·ª° n·ª£ c·ªßa kh√°ch h√†ng.\n",
    "\n",
    "---\n",
    "\n",
    "### A. Financial Ratios (TTM - Trailing 12 Months)\n",
    "\n",
    "C√°c t·ª∑ s·ªë t√†i ch√≠nh ƒë∆∞·ª£c t√≠nh to√°n d·ª±a tr√™n d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t (TTM) ƒë·ªÉ ph·∫£n √°nh xu h∆∞·ªõng d√†i h·∫°n v√† gi·∫£m thi·ªÉu ·∫£nh h∆∞·ªüng c·ªßa bi·∫øn ƒë·ªông ng·∫Øn h·∫°n. Ch√∫ng ta s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ 4 qu√Ω g·∫ßn nh·∫•t ƒë·ªÉ t√≠nh to√°n c√°c ch·ªâ s·ªë t·ªïng h·ª£p n√†y.\n",
    "\n",
    "#### Liquidity & Coverage Ratios\n",
    "\n",
    "**Interest Coverage Ratio (ICR)** l√† ch·ªâ s·ªë quan tr·ªçng nh·∫•t trong ƒë√°nh gi√° kh·∫£ nƒÉng tr·∫£ n·ª£, ƒë∆∞·ª£c t√≠nh b·∫±ng EBIT chia cho chi ph√≠ l√£i vay (Interest Expense). T·ª∑ s·ªë n√†y ƒëo l∆∞·ªùng kh·∫£ nƒÉng c·ªßa doanh nghi·ªáp trong vi·ªác tr·∫£ l√£i vay t·ª´ l·ª£i nhu·∫≠n ho·∫°t ƒë·ªông. Theo th√¥ng l·ªá ng√†nh ng√¢n h√†ng, ICR d∆∞·ªõi 1.5 ƒë∆∞·ª£c coi l√† m·ª©c nguy hi·ªÉm, cho th·∫•y doanh nghi·ªáp kh√¥ng ƒë·ªß kh·∫£ nƒÉng trang tr·∫£i nghƒ©a v·ª• l√£i vay t·ª´ thu nh·∫≠p ho·∫°t ƒë·ªông.\n",
    "\n",
    "**Debt Service Coverage Ratio (DSCR)** ƒëo l∆∞·ªùng kh·∫£ nƒÉng tr·∫£ c·∫£ n·ª£ g·ªëc v√† l√£i t·ª´ EBITDA sau khi tr·ª´ ƒëi chi ph√≠ v·ªën (CAPEX). Do d·ªØ li·ªáu synthetic kh√¥ng c√≥ th√¥ng tin chi ti·∫øt v·ªÅ kho·∫£n tr·∫£ n·ª£ g·ªëc, ch√∫ng ta s·ª≠ d·ª•ng proxy b·∫±ng c√°ch ∆∞·ªõc t√≠nh CAPEX l√† 30% c·ªßa EBITDA. DSCR d∆∞·ªõi 1.2 cho th·∫•y doanh nghi·ªáp g·∫∑p kh√≥ khƒÉn trong vi·ªác ƒë√°p ·ª©ng c√°c nghƒ©a v·ª• n·ª£.\n",
    "\n",
    "**Current Ratio** ph·∫£n √°nh thanh kho·∫£n ng·∫Øn h·∫°n, ƒë∆∞·ª£c t√≠nh b·∫±ng t√†i s·∫£n ng·∫Øn h·∫°n (Current Assets) chia cho n·ª£ ng·∫Øn h·∫°n (Current Liabilities). T·ª∑ s·ªë n√†y cho bi·∫øt kh·∫£ nƒÉng c·ªßa doanh nghi·ªáp trong vi·ªác thanh to√°n c√°c kho·∫£n n·ª£ ƒë·∫øn h·∫°n trong v√≤ng 12 th√°ng t·ªõi. Current Ratio d∆∞·ªõi 1.0 l√† d·∫•u hi·ªáu c·∫£nh b√°o thi·∫øu thanh kho·∫£n nghi√™m tr·ªçng.\n",
    "\n",
    "#### Leverage Ratio\n",
    "\n",
    "**Debt-to-EBITDA** ƒëo l∆∞·ªùng ƒë√≤n b·∫©y t√†i ch√≠nh, cho bi·∫øt doanh nghi·ªáp c·∫ßn bao nhi√™u nƒÉm EBITDA ƒë·ªÉ tr·∫£ h·∫øt n·ª£. T·ª∑ s·ªë n√†y ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªïng n·ª£ (Total Debt) chia cho EBITDA TTM. Theo chu·∫©n m·ª±c ng√†nh, Debt-to-EBITDA v∆∞·ª£t qu√° 4.0 cho th·∫•y doanh nghi·ªáp ƒëang ch·ªãu g√°nh n·∫∑ng n·ª£ qu√° m·ª©c, l√†m tƒÉng ƒë√°ng k·ªÉ r·ªßi ro v·ª° n·ª£.\n",
    "\n",
    "#### Working Capital Efficiency\n",
    "\n",
    "Nh√≥m ch·ªâ s·ªë n√†y ƒë√°nh gi√° hi·ªáu qu·∫£ qu·∫£n l√Ω v·ªën l∆∞u ƒë·ªông th√¥ng qua ba th√†nh ph·∫ßn ch√≠nh:\n",
    "\n",
    "**Days Sales Outstanding (DSO)** ƒëo l∆∞·ªùng s·ªë ng√†y trung b√¨nh ƒë·ªÉ thu h·ªìi ti·ªÅn t·ª´ kh√°ch h√†ng, ƒë∆∞·ª£c t√≠nh b·∫±ng (AR / Revenue) √ó 365. DSO tƒÉng cao cho th·∫•y doanh nghi·ªáp g·∫∑p kh√≥ khƒÉn trong vi·ªác thu h·ªìi c√¥ng n·ª£, c√≥ th·ªÉ d·∫´n ƒë·∫øn thi·∫øu h·ª•t ti·ªÅn m·∫∑t.\n",
    "\n",
    "**Days Payables Outstanding (DPO)** ƒëo l∆∞·ªùng s·ªë ng√†y trung b√¨nh doanh nghi·ªáp tr·∫£ ti·ªÅn cho nh√† cung c·∫•p, ƒë∆∞·ª£c t√≠nh b·∫±ng (AP / COGS) √ó 365. DPO cao c√≥ th·ªÉ l√† d·∫•u hi·ªáu t√≠ch c·ª±c (t·∫≠n d·ª•ng t√≠n d·ª•ng th∆∞∆°ng m·∫°i) ho·∫∑c ti√™u c·ª±c (kh√≥ khƒÉn thanh kho·∫£n).\n",
    "\n",
    "**Days On Hand (DOH)** ƒëo l∆∞·ªùng s·ªë ng√†y t·ªìn kho trung b√¨nh, ƒë∆∞·ª£c t√≠nh b·∫±ng (Inventory / COGS) √ó 365. DOH cao cho th·∫•y h√†ng t·ªìn kho nhi·ªÅu, c√≥ th·ªÉ l√†m gi√°n ƒëo·∫°n d√≤ng ti·ªÅn.\n",
    "\n",
    "**Cash Conversion Cycle (CCC)** l√† ch·ªâ s·ªë t·ªïng h·ª£p, ƒë∆∞·ª£c t√≠nh b·∫±ng DSO + DOH - DPO. CCC ƒëo l∆∞·ªùng s·ªë ng√†y v·ªën b·ªã \"ƒë√≥ng bƒÉng\" trong chu k·ª≥ kinh doanh, t·ª´ khi tr·∫£ ti·ªÅn mua h√†ng ƒë·∫øn khi thu ƒë∆∞·ª£c ti·ªÅn t·ª´ kh√°ch h√†ng. CCC tƒÉng cao cho th·∫•y hi·ªáu qu·∫£ qu·∫£n l√Ω v·ªën l∆∞u ƒë·ªông k√©m, l√†m d√≤ng ti·ªÅn x·∫•u ƒëi.\n",
    "\n",
    "#### Trend Analysis (QoQ)\n",
    "\n",
    "Ngo√†i c√°c ch·ªâ s·ªë tƒ©nh, ch√∫ng ta c√≤n t√≠nh to√°n xu h∆∞·ªõng thay ƒë·ªïi theo qu√Ω (Quarter-over-Quarter) cho c√°c ch·ªâ s·ªë quan tr·ªçng. **delta_dso_qoq** v√† **delta_ccc_qoq** cho bi·∫øt s·ª± thay ƒë·ªïi c·ªßa DSO v√† CCC so v·ªõi qu√Ω tr∆∞·ªõc, gi√∫p ph√°t hi·ªán s·ªõm c√°c d·∫•u hi·ªáu x·∫•u ƒëi trong qu·∫£n l√Ω v·ªën l∆∞u ƒë·ªông."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4b2b8",
   "metadata": {},
   "source": [
    "### B. Behavioral Features (Observation Window = 180 ng√†y)\n",
    "\n",
    "C√°c ƒë·∫∑c tr∆∞ng h√†nh vi ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ d·ªØ li·ªáu giao d·ªãch h√†ng ng√†y trong 180 ng√†y g·∫ßn nh·∫•t tr∆∞·ªõc ng√†y ƒë√°nh gi√° (as-of date). Nh·ªØng features n√†y ph·∫£n √°nh h√†nh vi s·ª≠ d·ª•ng t√≠n d·ª•ng th·ª±c t·∫ø c·ªßa kh√°ch h√†ng, th∆∞·ªùng c√≥ s·ª©c m·∫°nh d·ª± ƒëo√°n cao h∆°n so v·ªõi c√°c ch·ªâ s·ªë t√†i ch√≠nh truy·ªÅn th·ªëng v√¨ ch√∫ng n·∫Øm b·∫Øt ƒë∆∞·ª£c c√°c v·∫•n ƒë·ªÅ thanh kho·∫£n v√† kh√≥ khƒÉn t√†i ch√≠nh ngay khi ch√∫ng ph√°t sinh.\n",
    "\n",
    "#### Credit Utilization\n",
    "\n",
    "T·ª∑ l·ªá s·ª≠ d·ª•ng h·∫°n m·ª©c t√≠n d·ª•ng l√† ch·ªâ s·ªë quan tr·ªçng ph·∫£n √°nh m·ª©c ƒë·ªô ph·ª• thu·ªôc c·ªßa doanh nghi·ªáp v√†o ngu·ªìn v·ªën vay ng√¢n h√†ng. Ch√∫ng ta t√≠nh to√°n hai ch·ªâ s·ªë ch√≠nh:\n",
    "\n",
    "**%util_mean_60d** l√† t·ª∑ l·ªá s·ª≠ d·ª•ng h·∫°n m·ª©c trung b√¨nh trong 60 ng√†y g·∫ßn nh·∫•t, ƒë∆∞·ª£c t√≠nh b·∫±ng (Utilized / Limit) trung b√¨nh. Ch·ªâ s·ªë n√†y cho bi·∫øt m·ª©c ƒë·ªô \"cƒÉng\" v·ªÅ thanh kho·∫£n c·ªßa doanh nghi·ªáp. Utilization rate v∆∞·ª£t qu√° 85% cho th·∫•y doanh nghi·ªáp ƒëang √°p s√°t h·∫°n m·ª©c, c√≥ nguy c∆° thi·∫øu thanh kho·∫£n n·∫øu c√≥ b·∫•t k·ª≥ c√∫ s·ªëc n√†o.\n",
    "\n",
    "**%util_p95_60d** l√† percentile th·ª© 95 c·ªßa utilization trong 60 ng√†y, ƒëo l∆∞·ªùng ƒë·ªânh s·ª≠ d·ª•ng h·∫°n m·ª©c. Ch·ªâ s·ªë n√†y quan tr·ªçng v√¨ n√≥ cho th·∫•y trong nh·ªØng ng√†y \"x·∫•u nh·∫•t\", doanh nghi·ªáp s·ª≠ d·ª•ng bao nhi√™u ph·∫ßn trƒÉm h·∫°n m·ª©c, gi√∫p ph√°t hi·ªán c√°c giai ƒëo·∫°n cƒÉng th·∫≥ng thanh kho·∫£n t·∫°m th·ªùi.\n",
    "\n",
    "#### Delinquency Patterns\n",
    "\n",
    "Days Past Due (DPD) l√† ch·ªâ s·ªë tr·ª±c ti·∫øp nh·∫•t v·ªÅ kh√≥ khƒÉn thanh to√°n. Ch√∫ng ta ph√¢n t√≠ch DPD theo nhi·ªÅu g√≥c ƒë·ªô:\n",
    "\n",
    "**dpd_max_180d** l√† s·ªë ng√†y qu√° h·∫°n t·ªëi ƒëa trong 180 ng√†y qua. Theo chu·∫©n m·ª±c ng√†nh, DPD v∆∞·ª£t qu√° 30 ng√†y ƒë∆∞·ª£c coi l√† early warning signal, trong khi DPD v∆∞·ª£t 90 ng√†y l√† d·∫•u hi·ªáu r√µ r√†ng c·ªßa default risk theo ƒë·ªãnh nghƒ©a Basel.\n",
    "\n",
    "**dpd_trend_180d** ƒëo l∆∞·ªùng xu h∆∞·ªõng c·ªßa DPD theo th·ªùi gian b·∫±ng c√°ch t√≠nh slope (h·ªá s·ªë g√≥c) c·ªßa ƒë∆∞·ªùng h·ªìi quy tuy·∫øn t√≠nh gi·ªØa DPD v√† th·ªùi gian. Slope d∆∞∆°ng cho th·∫•y DPD ƒëang c√≥ xu h∆∞·ªõng tƒÉng d·∫ßn (t√¨nh h√¨nh x·∫•u ƒëi), trong khi slope √¢m cho th·∫•y doanh nghi·ªáp ƒëang c·∫£i thi·ªán kh·∫£ nƒÉng thanh to√°n.\n",
    "\n",
    "**near_due_freq_7d** ƒëo l∆∞·ªùng t·∫ßn su·∫•t \"g·∫ßn qu√° h·∫°n\" trong 7 ng√†y g·∫ßn nh·∫•t, ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† t·ª∑ l·ªá ng√†y c√≥ 0 < DPD < 30. Ch·ªâ s·ªë n√†y gi√∫p ph√°t hi·ªán c√°c kh√°ch h√†ng th∆∞·ªùng xuy√™n tr·ªÖ h·∫°n nh∆∞ng ch∆∞a ƒë·∫øn m·ª©c qu√° h·∫°n nghi√™m tr·ªçng, ƒë√¢y l√† early warning quan tr·ªçng.\n",
    "\n",
    "#### Credit Limit Breach\n",
    "\n",
    "**limit_breach_cnt_90d** ƒë·∫øm s·ªë l·∫ßn kh√°ch h√†ng v∆∞·ª£t qu√° h·∫°n m·ª©c t√≠n d·ª•ng trong 90 ng√†y g·∫ßn nh·∫•t. B·∫•t k·ª≥ l·∫ßn vi ph·∫°m n√†o (> 0) ƒë·ªÅu l√† d·∫•u hi·ªáu c·∫£nh b√°o nghi√™m tr·ªçng, cho th·∫•y doanh nghi·ªáp c√≥ nhu c·∫ßu v·ªën v∆∞·ª£t qu√° kh·∫£ nƒÉng ƒë∆∞·ª£c ph√™ duy·ªát, ho·∫∑c h·ªá th·ªëng ki·ªÉm so√°t n·ªôi b·ªô k√©m.\n",
    "\n",
    "---\n",
    "\n",
    "### C. Cashflow Features\n",
    "\n",
    "D√≤ng ti·ªÅn l√† \"huy·∫øt m·∫°ch\" c·ªßa doanh nghi·ªáp, quan tr·ªçng h∆°n c·∫£ l·ª£i nhu·∫≠n k·∫ø to√°n trong vi·ªác d·ª± ƒëo√°n kh·∫£ nƒÉng v·ª° n·ª£. Ch√∫ng ta ph√¢n t√≠ch d√≤ng ti·ªÅn v√†o/ra h√†ng ng√†y trong 180 ng√†y qua ƒë·ªÉ t·∫°o c√°c features:\n",
    "\n",
    "**inflow_mean_60d** v√† **outflow_mean_60d** l√† d√≤ng ti·ªÅn v√†o v√† ra trung b√¨nh trong 60 ng√†y g·∫ßn nh·∫•t. Hai ch·ªâ s·ªë n√†y ph·∫£n √°nh quy m√¥ v√† t√≠nh ·ªïn ƒë·ªãnh c·ªßa ho·∫°t ƒë·ªông kinh doanh. S·ª± ch√™nh l·ªách l·ªõn gi·ªØa inflow v√† outflow (burn rate cao) l√† d·∫•u hi·ªáu c·∫£nh b√°o.\n",
    "\n",
    "**inflow_drop_60d** ƒëo l∆∞·ªùng t·ª∑ l·ªá gi·∫£m c·ªßa d√≤ng ti·ªÅn v√†o trong 60 ng√†y g·∫ßn nh·∫•t so v·ªõi median 6 th√°ng, ƒë∆∞·ª£c t√≠nh b·∫±ng (median_6m - mean_60d) / median_6m. Ch·ªâ s·ªë n√†y gi√∫p ph√°t hi·ªán s·ªõm s·ª•t gi·∫£m doanh thu, m·ªôt trong nh·ªØng nguy√™n nh√¢n ch√≠nh d·∫´n ƒë·∫øn v·ª° n·ª£. Inflow drop v∆∞·ª£t qu√° 20% cho th·∫•y d√≤ng ti·ªÅn ƒëang gi·∫£m m·∫°nh, c·∫ßn c√≥ h√†nh ƒë·ªông can thi·ªáp ngay l·∫≠p t·ª©c.\n",
    "\n",
    "---\n",
    "\n",
    "### D. Covenant Breach Flags\n",
    "\n",
    "Covenant (ƒëi·ªÅu kho·∫£n r√†ng bu·ªôc) l√† c√°c ng∆∞·ª°ng t√†i ch√≠nh m√† kh√°ch h√†ng ph·∫£i duy tr√¨ theo h·ª£p ƒë·ªìng t√≠n d·ª•ng. Vi ph·∫°m covenant l√† early warning signal c·ª±c k·ª≥ quan tr·ªçng, th∆∞·ªùng x·∫£y ra tr∆∞·ªõc khi default th·ª±c s·ª± di·ªÖn ra.\n",
    "\n",
    "Ch√∫ng ta theo d√µi ba lo·∫°i covenant ch√≠nh: **breach_icr** (vi ph·∫°m ICR < 1.5), **breach_dscr** (vi ph·∫°m DSCR < 1.2), v√† **breach_leverage** (vi ph·∫°m Debt/EBITDA > 4.0). M·ªói breach flag l√† bi·∫øn nh·ªã ph√¢n (0/1), cho bi·∫øt kh√°ch h√†ng c√≥ vi ph·∫°m covenant t∆∞∆°ng ·ª©ng hay kh√¥ng. Vi ph·∫°m b·∫•t k·ª≥ covenant n√†o ƒë·ªÅu trigger c√°c h√†nh ƒë·ªông nh∆∞ renegotiation, tightening ƒëi·ªÅu ki·ªán, ho·∫∑c tƒÉng gi√°m s√°t.\n",
    "\n",
    "---\n",
    "\n",
    "### E. Normalization (Sector-Size)\n",
    "\n",
    "M·ªôt trong nh·ªØng th√°ch th·ª©c l·ªõn nh·∫•t trong credit scoring l√† so s√°nh c√°c doanh nghi·ªáp kh√°c nhau v·ªÅ quy m√¥ v√† ng√†nh ngh·ªÅ. M·ªôt SME trong ng√†nh Retail c√≥ ICR = 2.0 c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† t·ªët, nh∆∞ng c√πng ch·ªâ s·ªë ƒë√≥ v·ªõi m·ªôt Large Corporate trong ng√†nh IT l·∫°i l√† m·ª©c trung b√¨nh ho·∫∑c k√©m.\n",
    "\n",
    "ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, ch√∫ng ta √°p d·ª•ng **Z-score normalization** theo nh√≥m (Sector, Size_bucket). M·ªói feature ƒë∆∞·ª£c chu·∫©n h√≥a b·∫±ng c√°ch so s√°nh v·ªõi c√°c kh√°ch h√†ng c√πng ng√†nh v√† c√πng quy m√¥:\n",
    "\n",
    "```\n",
    "z_score = (value - median_group) / IQR_group\n",
    "```\n",
    "\n",
    "Ch√∫ng ta s·ª≠ d·ª•ng **Median v√† IQR (Interquartile Range)** thay v√¨ Mean v√† Standard Deviation v√¨ ch√∫ng robust h∆°n v·ªõi outliers, r·∫•t ph·ªï bi·∫øn trong d·ªØ li·ªáu t√†i ch√≠nh. Features sau khi normalize c√≥ suffix `__zs_sector_size`, v√≠ d·ª•: `icr_ttm__zs_sector_size`, `dpd_max_180d__zs_sector_size`.\n",
    "\n",
    "Normalization n√†y mang l·∫°i hai l·ª£i √≠ch quan tr·ªçng: (1) So s√°nh c√¥ng b·∫±ng gi·ªØa c√°c doanh nghi·ªáp c√πng ƒë·∫∑c ƒëi·ªÉm, v√† (2) TƒÉng s·ª©c m·∫°nh d·ª± ƒëo√°n c·ªßa model v√¨ features ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh theo context ri√™ng c·ªßa t·ª´ng nh√≥m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feature Engineering\n",
    "print(\"Command to create features:\")\n",
    "print(\"python src/feature_engineering.py --raw-dir data/raw --asof 2025-06-30 --outdir data/processed\")\n",
    "print(\"\\nKey features created:\")\n",
    "features = [\n",
    "    \"Financial: icr_ttm, dscr_ttm_proxy, debt_to_ebitda, current_ratio, dso, ccc\",\n",
    "    \"Behavioral: %util_mean_60d, dpd_max_180d, dpd_trend_180d, limit_breach_cnt_90d\",\n",
    "    \"Cashflow: inflow_mean_60d, inflow_drop_60d\",\n",
    "    \"Covenant: breach_icr, breach_dscr, breach_leverage\",\n",
    "    \"Normalized: *__zs_sector_size versions\"\n",
    "]\n",
    "for f in features:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1876c",
   "metadata": {},
   "source": [
    "## Step 3: Model Training & Calibration\n",
    "\n",
    "Module: `src/train_baseline.py`\n",
    "\n",
    "### Overview\n",
    "\n",
    "B∆∞·ªõc n√†y x√¢y d·ª±ng m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± ƒëo√°n x√°c su·∫•t v·ª° n·ª£ (PD) trong 12 th√°ng t·ªõi. Ch√∫ng ta s·ª≠ d·ª•ng **LightGBM** l√†m classifier c∆° s·ªü v√¨ kh·∫£ nƒÉng x·ª≠ l√Ω t·ªët nhi·ªÅu features, t·ª± ƒë·ªông h·ªçc ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá phi tuy·∫øn, v√† h·ªó tr·ª£ class balancing. Sau khi train, m√¥ h√¨nh ƒë∆∞·ª£c **calibrate** b·∫±ng Isotonic Regression ƒë·ªÉ ƒë·∫£m b·∫£o predicted probabilities ph·∫£n √°nh ƒë√∫ng true probabilities - ƒëi·ªÅu quan tr·ªçng cho credit risk management v√† tu√¢n th·ªß Basel.\n",
    "\n",
    "---\n",
    "\n",
    "### A. LightGBM Configuration\n",
    "\n",
    "LightGBM ƒë∆∞·ª£c ch·ªçn v√¨ x·ª≠ l√Ω t·ªët nhi·ªÅu features c√≥ quy m√¥ kh√°c nhau (financial ratios, utilization rates, DPD counts), t·ª± ƒë·ªông h·ªçc feature interactions (\"ICR th·∫•p + Utilization cao = R·ªßi ro cao\"), v√† h·ªó tr·ª£ class weighting cho imbalanced data (default rate ~5-10%).\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "```python\n",
    "LGBMClassifier(\n",
    "    n_estimators=300,           # 300 c√¢y trong ensemble\n",
    "    learning_rate=0.05,         # H·ªçc ch·∫≠m nh∆∞ng ·ªïn ƒë·ªãnh\n",
    "    num_leaves=15,              # Gi·ªõi h·∫°n complexity\n",
    "    max_depth=6,                # Tr√°nh overfitting\n",
    "    subsample=0.8,              # Row sampling (bagging)\n",
    "    colsample_bytree=0.8,       # Column sampling\n",
    "    min_child_samples=10,       # M·ªói leaf ‚â• 10 samples\n",
    "    reg_lambda=0.1,             # L2 regularization\n",
    "    scale_pos_weight=(1-pos_rate)/pos_rate,  # Auto-balance classes\n",
    "    objective='binary',\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "**Train-Test Split**: 80% training, 20% holdout test v·ªõi stratified sampling ƒë·ªÉ ƒë·∫£m b·∫£o default rate ƒë·ªìng ƒë·ªÅu.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Isotonic Calibration (CV=5)\n",
    "\n",
    "Gradient boosting models th∆∞·ªùng cho ra **uncalibrated probabilities** - khi model d·ª± ƒëo√°n PD = 20%, t·ª∑ l·ªá th·ª±c t·∫ø default c√≥ th·ªÉ l√† 15% ho·∫∑c 30%. Trong credit risk, ƒëi·ªÅu n√†y nguy hi·ªÉm v√¨ c√°c quy·∫øt ƒë·ªãnh quan tr·ªçng (capital allocation, pricing, provisioning) d·ª±a v√†o con s·ªë PD n√†y.\n",
    "\n",
    "**Isotonic Regression** l√† ph∆∞∆°ng ph√°p calibration non-parametric v√† monotonic, h·ªçc h√†m mapping t·ª´ raw probabilities sang calibrated probabilities v·ªõi r√†ng bu·ªôc ƒë∆°n ƒëi·ªáu tƒÉng (customer c√≥ raw PD cao h∆°n v·∫´n c√≥ calibrated PD cao h∆°n). Ch√∫ng ta s·ª≠ d·ª•ng 5-fold CV ƒë·ªÉ tr√°nh overfitting:\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    base_lgbm,\n",
    "    method='isotonic',\n",
    "    cv=5\n",
    ")\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Isotonic Regression ƒë∆∞·ª£c ∆∞u ti√™n h∆°n Platt Scaling v√¨: (1) Kh√¥ng gi·∫£ ƒë·ªãnh functional form (kh√¥ng c·∫ßn sigmoid), (2) ƒê·∫£m b·∫£o ranking kh√¥ng ƒë·ªïi, (3) Performance t·ªët h∆°n v·ªõi sample size l·ªõn (‚â• 1000 customers).\n",
    "\n",
    "---\n",
    "\n",
    "### C. Risk Tiers & Thresholds\n",
    "\n",
    "Sau calibration, customers ƒë∆∞·ª£c ph√¢n lo·∫°i v√†o 3 risk tiers d·ª±a tr√™n **percentile-based thresholds** (thay v√¨ absolute PD cutoffs) ƒë·ªÉ qu·∫£n l√Ω capacity. Ng√¢n h√†ng ch·ªâ c√≥ ƒë·ªß ngu·ªìn l·ª±c ƒë·ªÉ qu·∫£n l√Ω ch·∫∑t ch·∫Ω m·ªôt s·ªë l∆∞·ª£ng customers high-risk nh·∫•t, n√™n vi·ªác c·ªë ƒë·ªãnh Red tier = top 5% ƒë·∫£m b·∫£o s·ªë l∆∞·ª£ng customers c·∫ßn intensive monitoring kh√¥ng v∆∞·ª£t qu√° capacity.\n",
    "\n",
    "**Tier Definitions:**\n",
    "\n",
    "| Tier | Percentile | Typical PD | Action | Capacity |\n",
    "|------|-----------|-----------|--------|----------|\n",
    "| **Red** | Top 5% | ‚â• 20% | H·ªçp KH ‚â§5 ng√†y; l·∫≠p cash flow 13 tu·∫ßn; tighten covenants; watchlist | ~50 KH ‚Üí 5 RMs |\n",
    "| **Amber** | Top 5-15% | 5-20% | So√°t x√©t ‚â§10 ng√†y; y√™u c·∫ßu management accounts; h·∫°n ch·∫ø h·∫°n m·ª©c | ~100 KH ‚Üí 10 RMs |\n",
    "| **Green** | Bottom 85% | < 5% | Theo d√µi ƒë·ªãnh k·ª≥ quarterly; kh√¥ng c·∫ßn h√†nh ƒë·ªông ƒë·∫∑c bi·ªát | Portfolio monitoring |\n",
    "\n",
    "**Threshold Calculation:**\n",
    "\n",
    "```python\n",
    "train_probs = calibrated_model.predict_proba(X_train)[:, 1]\n",
    "red_threshold = np.percentile(train_probs, 95)      # e.g., 0.23\n",
    "amber_threshold = np.percentile(train_probs, 85)    # e.g., 0.08\n",
    "```\n",
    "\n",
    "Thresholds ƒë∆∞·ª£c l∆∞u v√†o `thresholds.json` v√† s·ª≠ d·ª•ng nh·∫•t qu√°n cho c√°c l·∫ßn scoring sau.\n",
    "\n",
    "---\n",
    "\n",
    "### D. Evaluation Metrics\n",
    "\n",
    "Model ƒë∆∞·ª£c ƒë√°nh gi√° to√†n di·ªán qua nhi·ªÅu metrics, m·ªói metric ƒëo l∆∞·ªùng m·ªôt kh√≠a c·∫°nh kh√°c nhau:\n",
    "\n",
    "**1. AUC-ROC (Discrimination Power)**  \n",
    "ƒêo kh·∫£ nƒÉng ph√¢n bi·ªát defaulters vs non-defaulters. AUC = 0.80 nghƒ©a l√† 80% tr∆∞·ªùng h·ª£p model s·∫Ω rank ƒë√∫ng (assign PD cao h∆°n cho defaulter). **Target**: ‚â• 0.75 (industry standard).\n",
    "\n",
    "**2. PR-AUC (Precision-Recall)**  \n",
    "Quan tr·ªçng v·ªõi imbalanced data (default rate th·∫•p). Precision = % customers ƒë∆∞·ª£c d·ª± ƒëo√°n default th·ª±c s·ª± default. Recall = % defaults th·ª±c t·∫ø ƒë∆∞·ª£c ph√°t hi·ªán. **Target**: ‚â• 0.40 (v·ªõi base rate ~8%).\n",
    "\n",
    "**3. KS Statistic (Kolmogorov-Smirnov)**  \n",
    "ƒêo maximum separation gi·ªØa cumulative distributions c·ªßa defaulters v√† non-defaulters. KS = max(TPR - FPR). **Target**: ‚â• 0.35 (good discriminatory power).\n",
    "\n",
    "**4. Brier Score (Calibration Quality)**  \n",
    "MSE c·ªßa probabilities: `Brier = (1/N) √ó Œ£(predicted_prob - actual_outcome)¬≤`. Brier nh·ªè nghƒ©a l√† predictions accurate (n·∫øu d·ª± ƒëo√°n 10 KH m·ªói ng∆∞·ªùi PD=20%, l√Ω t∆∞·ªüng c√≥ 2 defaults). **Target**: ‚â§ 0.10. Brier gi·∫£m ƒë√°ng k·ªÉ sau calibration (t·ª´ ~0.12 xu·ªëng ~0.08).\n",
    "\n",
    "**5. Calibration Curve (Reliability Diagram)**  \n",
    "Visualize calibration: plot mean predicted probability vs actual default rate trong t·ª´ng bin. ƒê∆∞·ªùng l√Ω t∆∞·ªüng l√† y = x (diagonal).\n",
    "\n",
    "---\n",
    "\n",
    "### E. Outputs & Artifacts\n",
    "\n",
    "**1. Model File**: `model_lgbm.pkl` - Ch·ª©a base LightGBM, calibrated model, feature names, v√† metadata (training date, hyperparameters, test AUC, test Brier).\n",
    "\n",
    "**2. Scores**: `scores_all.csv` - Predictions cho to√†n b·ªô dataset (train + test) v·ªõi columns: `customer_id`, `prob_default_12m_base`, `prob_default_12m_calibrated`, `tier`, `is_test`.\n",
    "\n",
    "**3. Thresholds**: `thresholds.json` - L∆∞u red/amber/green thresholds v√† percentiles ƒë·ªÉ d√πng cho scoring sau n√†y.\n",
    "\n",
    "**4. Visualizations**:\n",
    "- `calibration_lgbm.png`: Reliability diagram (before vs after calibration)\n",
    "- `pr_curve_lgbm.png`: Precision-Recall curve\n",
    "- `roc_curve_lgbm.png`: ROC curve  \n",
    "- `shap_summary.png`: Quick SHAP summary (top 10 features)\n",
    "\n",
    "T·∫•t c·∫£ artifacts ƒë∆∞·ª£c version control ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility v√† auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a43eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train model\n",
    "print(\"Command to train LightGBM model:\")\n",
    "print(\"python src/train_baseline.py --features data/processed/feature_ews.parquet --test-size 0.2 --seed 42 --red-pct 0.05 --amber-pct 0.10 --outdir artifacts/models\")\n",
    "print(\"\\nOutputs:\")\n",
    "print(\"  - artifacts/models/model_lgbm.pkl (base + calibrated model + features)\")\n",
    "print(\"  - artifacts/models/scores_all.csv (predictions + tiers)\")\n",
    "print(\"  - artifacts/models/thresholds.json\")\n",
    "print(\"  - artifacts/models/calibration_lgbm.png\")\n",
    "print(\"  - artifacts/models/pr_curve_lgbm.png\")\n",
    "print(\"  - artifacts/models/shap_summary.csv/png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a18bb",
   "metadata": {},
   "source": [
    "## üîç Step 4: Model Explainability (SHAP)\n",
    "\n",
    "Module: `src/explain.py`\n",
    "\n",
    "### Overview\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) gi·∫£i th√≠ch contribution c·ªßa t·ª´ng feature v√†o prediction d·ª±a tr√™n game theory. SHAP value d∆∞∆°ng nghƒ©a l√† feature ƒë√≥ tƒÉng x√°c su·∫•t default, √¢m nghƒ©a l√† gi·∫£m default risk, v√† magnitude cho bi·∫øt m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng. ƒêi·ªÅu n√†y quan tr·ªçng cho Credit Committee (gi·∫£i th√≠ch decisions), RM Team (t∆∞ v·∫•n customers c·∫£i thi·ªán), v√† Model Validation (ƒë·∫£m b·∫£o model h·ªçc ƒë√∫ng patterns).\n",
    "\n",
    "---\n",
    "\n",
    "### Global Explainability\n",
    "\n",
    "**Feature Importance** (`feature_importance.csv`): Mean absolute SHAP values cho m·ªói feature, cho bi·∫øt features n√†o ·∫£nh h∆∞·ªüng nh·∫•t ƒë·∫øn model trong to√†n b·ªô portfolio. V√≠ d·ª•, `dpd_max_180d__zs_sector_size` th∆∞·ªùng l√† feature quan tr·ªçng nh·∫•t v√¨ DPD l√† signal m·∫°nh nh·∫•t cho default risk.\n",
    "\n",
    "**SHAP Summary Plot** (`shap_summary.png`): Waterfall plot visualize impact c·ªßa t·∫•t c·∫£ features. M·ªói ƒëi·ªÉm l√† m·ªôt customer, m√†u ƒë·ªè = feature value cao, xanh = feature value th·∫•p. Plot n√†y cho th·∫•y kh√¥ng ch·ªâ feature n√†o quan tr·ªçng m√† c√≤n direction c·ªßa impact (high DPD ‚Üí high risk, high ICR ‚Üí low risk).\n",
    "\n",
    "---\n",
    "\n",
    "### Local Explainability\n",
    "\n",
    "**Top Drivers per Customer** (`top_drivers_per_customer.csv`): Top 5 features quan tr·ªçng nh·∫•t cho t·ª´ng customer c·ª• th·ªÉ, gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi \"T·∫°i sao customer C0042 ƒë∆∞·ª£c ph√¢n v√†o Red tier?\". Output bao g·ªìm feature name, SHAP value, v√† actual feature value.\n",
    "\n",
    "V√≠ d·ª• cho customer C0042:\n",
    "1. `dpd_max_180d__zs_sector_size`: SHAP = +0.52 (value = 120 days) ‚Üí DPD cao\n",
    "2. `%util_mean_60d__zs_sector_size`: SHAP = +0.31 (value = 0.95) ‚Üí Utilization s√°t h·∫°n m·ª©c\n",
    "3. `icr_ttm__zs_sector_size`: SHAP = +0.20 (value = 0.8) ‚Üí ICR th·∫•p, kh√≥ tr·∫£ l√£i\n",
    "\n",
    "V·ªõi th√¥ng tin n√†y, RM c√≥ th·ªÉ t∆∞ v·∫•n customer: (1) Clear outstanding payments ƒë·ªÉ gi·∫£m DPD, (2) Gi·∫£m credit usage ho·∫∑c apply for limit increase, (3) C·∫£i thi·ªán profitability ho·∫∑c restructure debt.\n",
    "\n",
    "---\n",
    "\n",
    "### Dependence Plots\n",
    "\n",
    "SHAP dependence plots cho key features (`icr_ttm`, `ccc`, `%util_mean_60d`) hi·ªÉn th·ªã m·ªëi quan h·ªá phi tuy·∫øn gi·ªØa feature value v√† SHAP value. V√≠ d·ª•, dependence plot c·ªßa ICR c√≥ th·ªÉ cho th·∫•y: ICR < 1.5 c√≥ SHAP values r·∫•t cao (risk tƒÉng m·∫°nh), ICR 1.5-3.0 c√≥ SHAP gi·∫£m d·∫ßn, ICR > 3.0 c√≥ SHAP g·∫ßn 0 (kh√¥ng c√≤n r·ªßi ro th√™m). Nh·ªØng insights n√†y gi√∫p validate model ƒëang h·ªçc ƒë√∫ng business logic.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs Summary\n",
    "\n",
    "| File | Type | Purpose |\n",
    "|------|------|---------|\n",
    "| `feature_importance.csv` | Global | Ranking features by importance |\n",
    "| `shap_summary.png` | Global | Visual impact of all features |\n",
    "| `top_drivers_per_customer.csv` | Local | Top 5 drivers cho t·ª´ng customer |\n",
    "| `shap_dependence_*.png` | Global | Phi tuy·∫øn relationships |\n",
    "| `summary.json` | Metadata | Config v√† stats |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66152533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate SHAP explanations\n",
    "print(\"Command to generate SHAP explanations:\")\n",
    "print(\"python src/explain.py --model artifacts/models/model_lgbm.pkl --features data/processed/feature_ews.parquet --outdir artifacts/shap --max-display 20\")\n",
    "print(\"\\nOutputs:\")\n",
    "print(\"  - artifacts/shap/feature_importance.csv\")\n",
    "print(\"  - artifacts/shap/shap_summary.png\")\n",
    "print(\"  - artifacts/shap/top_drivers_per_customer.csv\")\n",
    "print(\"  - artifacts/shap/shap_dependence_*.png\")\n",
    "print(\"  - artifacts/shap/summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121d40f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5-6: [Optional] Re-calibration\n",
    "\n",
    "### Why Optional?\n",
    "\n",
    "B∆∞·ªõc 3 (train_baseline.py) ƒë√£ t·∫°o ra m·ªôt **calibrated model** v·ªõi percentile-based thresholds (Red = top 5%, Amber = top 5-15%) s·∫µn s√†ng cho production. Steps 5-6 ch·ªâ c·∫ßn thi·∫øt khi business mu·ªën **thay ƒë·ªïi threshold strategy** t·ª´ percentile-based sang **absolute PD cutoffs** (v√≠ d·ª•: Red ‚â• 20% PD, Amber ‚â• 5% PD) ƒë·ªÉ ph√π h·ª£p v·ªõi risk appetite ho·∫∑c regulatory requirements c·ª• th·ªÉ.\n",
    "\n",
    "Trong th·ª±c t·∫ø, percentile-based approach th∆∞·ªùng ƒë∆∞·ª£c ∆∞u ti√™n v√¨ ƒë·∫£m b·∫£o s·ªë l∆∞·ª£ng customers c·∫ßn intensive monitoring kh√¥ng v∆∞·ª£t qu√° capacity. Tuy nhi√™n, m·ªôt s·ªë t·ªï ch·ª©c (ƒë·∫∑c bi·ªát banks tu√¢n th·ªß Basel/IFRS 9) y√™u c·∫ßu absolute thresholds ƒë·ªÉ nh·∫•t qu√°n v·ªõi internal risk rating systems ho·∫∑c regulatory reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Extract Raw Scores\n",
    "\n",
    "**Module**: `src/make_scores_raw.py`\n",
    "\n",
    "Tr√≠ch xu·∫•t raw probabilities t·ª´ **base LightGBM** (tr∆∞·ªõc khi √°p d·ª•ng isotonic calibration trong Step 3) ƒë·ªÉ c√≥ baseline scores cho re-calibration process. Output l√† `scores_raw.csv` ch·ª©a uncalibrated predictions cho to√†n b·ªô dataset.\n",
    "\n",
    "**Why needed?** Re-calibration c·∫ßn raw scores l√†m input v√¨ ch√∫ng ta s·∫Ω fit m·ªôt calibrator m·ªõi v·ªõi absolute thresholds kh√°c v·ªõi calibrator trong Step 3.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Re-calibrate with Absolute Thresholds\n",
    "\n",
    "**Module**: `src/calibrate.py`\n",
    "\n",
    "Fit l·∫°i **Isotonic Regression** tr√™n raw scores v·ªõi absolute PD cutoffs thay v√¨ percentiles. Process bao g·ªìm: (1) Fit calibrator tr√™n training set, (2) Map raw scores ‚Üí calibrated PD, (3) Apply absolute thresholds (Red ‚â• 20%, Amber ‚â• 5%), (4) Save calibrator v√† thresholds.\n",
    "\n",
    "**Key difference from Step 3:**\n",
    "- Step 3: Calibrate ‚Üí Calculate percentile thresholds ‚Üí Tiers fixed by % (top 5%, 10%)\n",
    "- Step 6: Calibrate ‚Üí Apply absolute PD thresholds ‚Üí Tiers vary by portfolio quality\n",
    "\n",
    "**Outputs:**\n",
    "- `calibrator.pkl`: New isotonic calibrator\n",
    "- `mapping.csv`: Raw score ‚Üí Calibrated PD mapping table\n",
    "- `thresholds.json`: Absolute cutoffs (red: 0.20, amber: 0.05)\n",
    "- `calibration_full.png`: Reliability diagram\n",
    "- `pr_curve_full.png`: Precision-Recall curve\n",
    "\n",
    "**Tradeoff:** V·ªõi absolute thresholds, s·ªë l∆∞·ª£ng customers trong Red/Amber tiers c√≥ th·ªÉ bi·∫øn ƒë·ªông theo quality c·ªßa portfolio (good period ‚Üí √≠t Red, bad period ‚Üí nhi·ªÅu Red), g√¢y kh√≥ khƒÉn cho capacity planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3385969",
   "metadata": {},
   "source": [
    "## üéØ Step 7: Production Scoring\n",
    "\n",
    "Module: `src/scoring.py`\n",
    "\n",
    "Scoring l√† b∆∞·ªõc cu·ªëi c√πng ƒë·ªÉ ƒë∆∞a model v√†o production. Script n√†y load trained model, predict PD cho to√†n b·ªô customers d·ª±a tr√™n feature snapshot t·∫°i as-of date (v√≠ d·ª•: 2025-06-30), sau ƒë√≥ ph√¢n tier v√† ƒë∆∞a ra action recommendations. Output ƒë∆∞·ª£c s·ª≠ d·ª•ng tr·ª±c ti·∫øp b·ªüi RM team v√† Risk Committee ƒë·ªÉ ra quy·∫øt ƒë·ªãnh nghi·ªáp v·ª•.\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs & Outputs\n",
    "\n",
    "**Inputs:**\n",
    "1. **Features**: `data/processed/feature_ews.parquet` - Feature snapshot t·∫°i as-of date\n",
    "2. **Model**: `artifacts/models/model_lgbm.pkl` - Trained & calibrated LightGBM\n",
    "3. **Thresholds**: `artifacts/calibration/thresholds.json` ho·∫∑c `artifacts/models/thresholds.json` - T√πy approach (absolute vs percentile)\n",
    "\n",
    "**Output**: `ews_scored_YYYY-MM-DD.csv` v·ªõi columns:\n",
    "\n",
    "| Column | Description | Example |\n",
    "|--------|-------------|---------|\n",
    "| `customer_id` | Customer identifier | C0042 |\n",
    "| `prob_default_12m_calibrated` | PD trong 12 th√°ng (0-1) | 0.2341 |\n",
    "| `score_ews` | EWS Score (0-100) | 76.59 |\n",
    "| `tier` | Risk tier | Red |\n",
    "| `action` | Recommended action | H·ªçp KH ‚â§5 ng√†y; l·∫≠p cash flow 13 tu·∫ßn;... |\n",
    "\n",
    "**EWS Score Formula**: `100 √ó (1 - PD)` ‚Üí Score cao = R·ªßi ro th·∫•p (100 = t·ªët nh·∫•t, 0 = x·∫•u nh·∫•t)\n",
    "\n",
    "---\n",
    "\n",
    "### Risk Tiers & Actions\n",
    "\n",
    "| Tier | Criteria | Action | Frequency |\n",
    "|------|----------|--------|-----------|\n",
    "| **Green** | PD < 5% (ho·∫∑c bottom 85%) | Theo d√µi ƒë·ªãnh k·ª≥; c·∫≠p nh·∫≠t BCTC ƒë√∫ng h·∫°n | Quarterly |\n",
    "| **Amber** | 5% ‚â§ PD < 20% (ho·∫∑c top 5-15%) | So√°t x√©t RM ‚â§10 ng√†y; y√™u c·∫ßu management accounts; ki·ªÉm tra c√¥ng n·ª£; h·∫°n ch·∫ø h·∫°n m·ª©c | Monthly |\n",
    "| **Red** | PD ‚â• 20% (ho·∫∑c top 5%) | H·ªçp KH ‚â§5 ng√†y; l·∫≠p cash flow 13 tu·∫ßn; xem x√©t covenant tightening/collateral; watchlist | Weekly |\n",
    "\n",
    "**Note**: Criteria ph·ª• thu·ªôc v√†o threshold approach (absolute vs percentile) ƒë∆∞·ª£c ch·ªçn ·ªü Steps 3 ho·∫∑c 5-6.\n",
    "\n",
    "---\n",
    "\n",
    "### Production Workflow\n",
    "\n",
    "**Monthly Cadence**:\n",
    "1. **Last day of month**: Ch·∫°y scoring script v·ªõi as-of date = month-end\n",
    "2. **Day 1-2**: Ph√¢n ph·ªëi report cho RM team v√† Risk Committee\n",
    "3. **Day 3-10**: RMs th·ª±c hi·ªán actions theo tier (Amber reviews, Red meetings)\n",
    "4. **Throughout month**: Track action completion v√† update customer status\n",
    "\n",
    "**Integration v·ªõi Banking Systems**:\n",
    "- **Input**: Features t·ª´ core banking system (financial data, credit transactions, cashflow)\n",
    "- **Output**: EWS scores import v√†o CRM/Credit Risk systems\n",
    "- **Alerts**: Auto-trigger emails/notifications cho customers chuy·ªÉn sang Red tier\n",
    "\n",
    "**Monitoring**: Track tier migrations month-over-month ƒë·ªÉ identify portfolio trends (improving/deteriorating)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6774e35",
   "metadata": {},
   "source": [
    "## üìä Model Performance & Validation\n",
    "\n",
    "### Expected Performance Metrics (Holdout 20%)\n",
    "\n",
    "Model ƒë∆∞·ª£c ƒë√°nh gi√° tr√™n holdout test set v·ªõi c√°c metrics sau (computed trong `train_baseline.py` lines 99-104):\n",
    "\n",
    "| Metric | Target Range | √ù nghƒ©a | Code |\n",
    "|--------|-------------|---------|------|\n",
    "| **AUC-ROC** | 0.75 - 0.85 | Kh·∫£ nƒÉng ph√¢n bi·ªát defaulters vs non-defaulters | `roc_auc_score(y_te, p_te)` |\n",
    "| **PR-AUC** | 0.40 - 0.60 | Performance tr√™n positive class (quan tr·ªçng v·ªõi imbalanced data) | `average_precision_score(y_te, p_te)` |\n",
    "| **KS Statistic** | 0.35 - 0.50 | Maximum separation gi·ªØa cumulative distributions | `ks_score(y_te, p_te)` |\n",
    "| **Brier Score** | 0.05 - 0.10 | Calibration quality (lower is better) | `brier_score_loss(y_te, p_te)` |\n",
    "\n",
    "**Calibration Quality**: Reliability curve (predicted probabilities vs actual default rates) n√™n g·∫ßn diagonal (y = x). Isotonic calibration c·∫£i thi·ªán ƒë√°ng k·ªÉ metric n√†y, th∆∞·ªùng gi·∫£m Brier score t·ª´ ~0.12 xu·ªëng ~0.08. Plots ƒë∆∞·ª£c generate trong `plot_calibration_pr()` function (lines 47-61).\n",
    "\n",
    "**Precision-Recall Tradeoff**: Red threshold (PD ‚â• 20%) c√≥ high precision, moderate recall; Amber threshold (PD ‚â• 5%) c√≥ balanced precision-recall.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Monitoring & Maintenance\n",
    "\n",
    "**Quarterly Reviews** (c·∫ßn t·ª± implement monitoring scripts):\n",
    "1. **Performance drift**: Monitor AUC, KS tr√™n new data (target: kh√¥ng gi·∫£m > 5%)\n",
    "   - Re-run `train_baseline.py` tr√™n new data v√† compare metrics\n",
    "2. **Population Stability Index (PSI)**: ƒêo distribution shift c·ªßa features (target: PSI < 0.15)\n",
    "   - Formula: `PSI = Œ£(actual% - expected%) √ó ln(actual%/expected%)`\n",
    "3. **Feature stability**: Check data quality, missing values, outliers\n",
    "   - S·ª≠ d·ª•ng data profiling tools ho·∫∑c pandas `.describe()`\n",
    "4. **Recalibration**: N·∫øu Brier score tƒÉng > 0.10, consider re-fit calibrator\n",
    "   - Re-run Step 6 (`calibrate.py`) v·ªõi data m·ªõi\n",
    "\n",
    "**Red Flags Trigger Retraining**:\n",
    "- AUC drops below 0.70\n",
    "- Brier score > 0.15\n",
    "- Large prediction shifts without business explanation (e.g., 10% customers chuy·ªÉn tier b·∫•t th∆∞·ªùng)\n",
    "- PSI > 0.25 (severe distribution shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe319a76",
   "metadata": {},
   "source": [
    "## Feature Importance Ranking\n",
    "\n",
    "D·ª±a tr√™n SHAP analysis trong `explain.py`, ƒë√¢y l√† 10 features c√≥ impact m·∫°nh nh·∫•t ƒë·∫øn d·ª± b√°o default:\n",
    "\n",
    "| # | Feature Name | Category | Business Interpretation |\n",
    "|---|--------------|----------|------------------------|\n",
    "| 1 | `dpd_max_180d__zs_sector_size` | Behavioral | DPD t·ªëi ƒëa trong 6 th√°ng - signal m·∫°nh nh·∫•t cho default risk |\n",
    "| 2 | `%util_mean_60d__zs_sector_size` | Behavioral | Credit utilization trung b√¨nh - ph·∫£n √°nh liquidity stress |\n",
    "| 3 | `icr_ttm__zs_sector_size` | Financial | Interest Coverage Ratio - kh·∫£ nƒÉng tr·∫£ l√£i vay |\n",
    "| 4 | `debt_to_ebitda__zs_sector_size` | Financial | Financial leverage - m·ª©c ƒë·ªô ƒë√≤n b·∫©y t√†i ch√≠nh |\n",
    "| 5 | `ccc__zs_sector_size` | Financial | Cash Conversion Cycle - hi·ªáu qu·∫£ qu·∫£n l√Ω v·ªën l∆∞u ƒë·ªông |\n",
    "| 6 | `inflow_drop_60d__zs_sector_size` | Cashflow | M·ª©c gi·∫£m doanh thu - suy gi·∫£m cashflow |\n",
    "| 7 | `dpd_trend_180d__zs_sector_size` | Behavioral | Xu h∆∞·ªõng DPD tƒÉng - payment behavior ƒëang x·∫•u ƒëi |\n",
    "| 8 | `breach_icr` | Covenant | Vi ph·∫°m covenant ICR - trigger event tr·ª±c ti·∫øp |\n",
    "| 9 | `current_ratio__zs_sector_size` | Financial | Current Ratio < 1.0 - nguy c∆° thanh kho·∫£n ng·∫Øn h·∫°n |\n",
    "| 10 | `delta_ccc_qoq__zs_sector_size` | Financial | Thay ƒë·ªïi CCC theo qu√Ω - efficiency ƒëang gi·∫£m |\n",
    "\n",
    "### Ph√¢n t√≠ch theo Category\n",
    "\n",
    "- **Behavioral (40%)**: Payment patterns th·ª±c t·∫ø l√† predictor m·∫°nh nh·∫•t - DPD history v√† utilization cho signal s·ªõm nh·∫•t v·ªÅ kh√≥ khƒÉn t√†i ch√≠nh\n",
    "- **Financial (35%)**: C√°c ch·ªâ s·ªë t√†i ch√≠nh fundamental (ICR, leverage, liquidity ratios) quan tr·ªçng th·ª© hai\n",
    "- **Cashflow (15%)**: Revenue trends v√† cashflow dynamics detect deterioration s·ªõm h∆°n b√°o c√°o t√†i ch√≠nh\n",
    "- **Covenant (10%)**: Breach events c√≥ impact ƒë√°ng k·ªÉ nh∆∞ng xu·∫•t hi·ªán mu·ªôn h∆°n\n",
    "\n",
    "**K·∫øt lu·∫≠n**: Model ∆∞u ti√™n behavioral signals v√¨ payment difficulties xu·∫•t hi·ªán tr∆∞·ªõc khi financial statements ph·∫£n √°nh ƒë·∫ßy ƒë·ªß. ƒêi·ªÅu n√†y ph√π h·ª£p v·ªõi th·ª±c t·∫ø risk management trong credit monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4792cd",
   "metadata": {},
   "source": [
    "## üöÄ Complete End-to-End Pipeline\n",
    "\n",
    "### Full Workflow (Development)\n",
    "\n",
    "```bash\n",
    "# Step 1: Generate synthetic data\n",
    "python src/generate_data.py --n-customers 1000 --output-dir data/raw\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "python src/feature_engineering.py --raw-dir data/raw --asof 2025-06-30 --outdir data/processed\n",
    "\n",
    "# Step 3: Train model + calibration\n",
    "python src/train_baseline.py --features data/processed/feature_ews.parquet --test-size 0.2 --seed 42 --red-pct 0.05 --amber-pct 0.10 --outdir artifacts/models\n",
    "\n",
    "# Step 4: Generate SHAP explanations\n",
    "python src/explain.py --model artifacts/models/model_lgbm.pkl --features data/processed/feature_ews.parquet --outdir artifacts/shap --max-display 20\n",
    "\n",
    "# [Optional] Step 5-6: Re-calibration with absolute thresholds\n",
    "python src/make_scores_raw.py --features data/processed/feature_ews.parquet --model artifacts/models/model_lgbm.pkl --out data/processed/scores_raw.csv\n",
    "python src/calibrate.py --input data/processed/scores_raw.csv --red-thr 0.20 --amber-thr 0.05 --outdir artifacts/calibration\n",
    "\n",
    "# Step 7: Production scoring\n",
    "python src/scoring.py --features data/processed/feature_ews.parquet --model artifacts/models/model_lgbm.pkl --thresholds artifacts/calibration/thresholds.json --asof 2025-06-30 --outdir artifacts/scoring\n",
    "```\n",
    "\n",
    "### Or use Makefile (if configured)\n",
    "\n",
    "```bash\n",
    "make requirements    # Install dependencies\n",
    "make lint           # Check code quality\n",
    "make format         # Format code with ruff\n",
    "make test          # Run pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700863f",
   "metadata": {},
   "source": [
    "## üíº Business Use Cases\n",
    "\n",
    "### 1. Portfolio Review Meeting (Monthly)\n",
    "\n",
    "**Input:** `ews_scored_YYYY-MM-DD.csv`\n",
    "\n",
    "**Analysis:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "scores = pd.read_csv('artifacts/scoring/ews_scored_2025-06-30.csv')\n",
    "\n",
    "# Portfolio distribution\n",
    "print(scores['tier'].value_counts())\n",
    "# Green: 850 customers (85%)\n",
    "# Amber: 100 customers (10%)\n",
    "# Red:    50 customers (5%)\n",
    "\n",
    "# High-risk customers requiring immediate action\n",
    "red_tier = scores[scores['tier'] == 'Red'].sort_values('prob_default_12m_calibrated', ascending=False)\n",
    "print(f\"Red tier: {len(red_tier)} customers with avg PD = {red_tier['prob_default_12m_calibrated'].mean():.1%}\")\n",
    "```\n",
    "\n",
    "**Actions:**\n",
    "- **Red tier:** Immediate RM meeting + action plan\n",
    "- **Amber tier:** Enhanced monitoring + covenant review\n",
    "- **Green tier:** Standard periodic review\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Credit Approval Process\n",
    "\n",
    "**New loan application t·ª´ kh√°ch h√†ng C0523:**\n",
    "\n",
    "```python\n",
    "# Get customer's EWS score\n",
    "customer = scores[scores['customer_id'] == 'C0523'].iloc[0]\n",
    "print(f\"Customer C0523:\")\n",
    "print(f\"  - EWS Score: {customer['score_ews']}\")\n",
    "print(f\"  - PD 12M: {customer['prob_default_12m_calibrated']:.1%}\")\n",
    "print(f\"  - Tier: {customer['tier']}\")\n",
    "print(f\"  - Action: {customer['action']}\")\n",
    "\n",
    "# Decision rule\n",
    "if customer['tier'] == 'Red':\n",
    "    print(\"REJECT or require additional collateral\")\n",
    "elif customer['tier'] == 'Amber':\n",
    "    print(\"APPROVE with covenant tightening\")\n",
    "else:\n",
    "    print(\"APPROVE standard terms\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Early Intervention\n",
    "\n",
    "**Identify deteriorating customers:**\n",
    "\n",
    "```python\n",
    "# Compare current month vs last month\n",
    "current = pd.read_csv('artifacts/scoring/ews_scored_2025-06-30.csv')\n",
    "previous = pd.read_csv('artifacts/scoring/ews_scored_2025-05-31.csv')\n",
    "\n",
    "merged = current.merge(previous, on='customer_id', suffixes=('_current', '_previous'))\n",
    "merged['pd_change'] = merged['prob_default_12m_calibrated_current'] - merged['prob_default_12m_calibrated_previous']\n",
    "\n",
    "# Customers with PD increasing by > 10pp\n",
    "deteriorating = merged[merged['pd_change'] > 0.10].sort_values('pd_change', ascending=False)\n",
    "print(f\"Deteriorating customers: {len(deteriorating)}\")\n",
    "```\n",
    "\n",
    "**Trigger actions:**\n",
    "- Request updated financials\n",
    "- Schedule RM meeting\n",
    "- Review credit limits\n",
    "\n",
    "---\n",
    "\n",
    "### 4. SHAP-based Customer Advisory\n",
    "\n",
    "**Why is customer C0042 in Red tier?**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load SHAP drivers\n",
    "drivers = pd.read_csv('artifacts/shap/top_drivers_per_customer.csv')\n",
    "customer_drivers = drivers[drivers['customer_id'] == 'C0042'].iloc[0]\n",
    "\n",
    "print(\"Top 3 risk drivers:\")\n",
    "for i in range(1, 4):\n",
    "    print(f\"{i}. {customer_drivers[f'feat{i}']}: {customer_drivers[f'shap{i}']:.3f} (value={customer_drivers[f'value{i}']})\")\n",
    "\n",
    "# Output:\n",
    "# 1. dpd_max_180d__zs_sector_size: 0.523 (value=120)\n",
    "# 2. %util_mean_60d__zs_sector_size: 0.312 (value=0.95)\n",
    "# 3. icr_ttm__zs_sector_size: 0.201 (value=0.8)\n",
    "```\n",
    "\n",
    "**RM Advice to customer:**\n",
    "1. **DPD 120 days:** Clear outstanding payments immediately\n",
    "2. **95% utilization:** Reduce credit line usage or apply for limit increase\n",
    "3. **ICR 0.8:** Improve profitability or restructure debt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1732f",
   "metadata": {},
   "source": [
    "## üìà Artifacts & Outputs Summary\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "artifacts/\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model_lgbm.pkl              # Trained model (base + calibrated + features)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ scores_all.csv              # Training set predictions + tiers\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ thresholds.json             # Percentile-based thresholds\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ calibration_lgbm.png        # Reliability diagram\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ pr_curve_lgbm.png           # Precision-Recall curve\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ shap_summary.csv/png        # Quick SHAP summary\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ calibration/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ calibrator.pkl              # Isotonic calibrator (re-fitted)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mapping.csv                 # Raw score ‚Üí Calibrated PD mapping\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ thresholds.json             # Absolute PD thresholds (Red ‚â•20%, Amber ‚â•5%)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ calibration_full.png        # Reliability curve (re-calibrated)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ pr_curve_full.png           # PR curve (re-calibrated)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ shap/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.csv      # Global feature importance (mean |SHAP|)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ shap_summary.png            # SHAP waterfall plot\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ top_drivers_per_customer.csv # Local explanations (top 5 features per customer)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ shap_dependence_*.png       # Dependence plots for key features\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ summary.json                # Metadata\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ scoring/\n",
    "    ‚îú‚îÄ‚îÄ ews_scored_2025-06-30.csv   # Production scores (customer_id, PD, score, tier, action)\n",
    "    ‚îî‚îÄ‚îÄ thresholds_used.json        # Thresholds applied in this run\n",
    "```\n",
    "\n",
    "### Key Files for Different Stakeholders\n",
    "\n",
    "| Stakeholder | Key Files |\n",
    "|-------------|-----------|\n",
    "| **Risk Manager** | `ews_scored_*.csv`, `top_drivers_per_customer.csv` |\n",
    "| **Credit Committee** | `scores_all.csv`, `shap_summary.png`, `pr_curve_lgbm.png` |\n",
    "| **Data Scientist** | `model_lgbm.pkl`, `feature_importance.csv`, all plots |\n",
    "| **Model Validator** | `calibration_*.png`, `thresholds.json`, metrics in console output |\n",
    "| **Auditor** | All artifacts + `summary.json` for traceability |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b992b",
   "metadata": {},
   "source": [
    "## üî¨ Technical Deep Dives\n",
    "\n",
    "### 1. Why Isotonic Calibration?\n",
    "\n",
    "**Problem with raw LightGBM probabilities:**\n",
    "- Overconfident near 0 and 1\n",
    "- Not well-calibrated for credit risk (regulatory requirement)\n",
    "\n",
    "**Isotonic Regression:**\n",
    "- Non-parametric, monotonic calibration\n",
    "- Preserves ranking (AUC unchanged)\n",
    "- Improves Brier score and reliability\n",
    "\n",
    "**Alternative: Platt Scaling (Logistic Regression)**\n",
    "- Parametric (assumes sigmoid relationship)\n",
    "- Less flexible than Isotonic\n",
    "- Use if you need smooth curve\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Class Imbalance Handling\n",
    "\n",
    "**Default rate ~5-10%** ‚Üí Highly imbalanced\n",
    "\n",
    "**Strategies applied:**\n",
    "1. **`scale_pos_weight`** in LightGBM\n",
    "   - Automatically weights positive class\n",
    "   - Formula: `(n_negative / n_positive)`\n",
    "   \n",
    "2. **Evaluation metrics:** PR-AUC instead of just ROC-AUC\n",
    "   - ROC-AUC can be misleading with imbalanced data\n",
    "   \n",
    "3. **Threshold tuning:** Separate from 0.5\n",
    "   - Red/Amber thresholds based on business capacity\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Feature Normalization (Sector-Size)\n",
    "\n",
    "**Why normalize by (Sector, Size)?**\n",
    "\n",
    "```python\n",
    "# Example: ICR = 2.0 for a SME in Retail\n",
    "# Is this good or bad?\n",
    "\n",
    "# Without normalization: Compare to all companies ‚Üí Looks average\n",
    "# With sector-size normalization: Compare to SME Retailers ‚Üí Looks good!\n",
    "\n",
    "# Implementation:\n",
    "def sector_size_normalize(df, cols):\n",
    "    for c in cols:\n",
    "        grouped = df.groupby(['sector_code', 'size_bucket'])\n",
    "        median = grouped[c].transform('median')\n",
    "        iqr = grouped[c].transform(lambda x: x.quantile(0.75) - x.quantile(0.25))\n",
    "        df[f'{c}__zs_sector_size'] = (df[c] - median) / iqr\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Fair comparison (SME vs SME, Corp vs Corp, same sector)\n",
    "- Robust to outliers (median/IQR instead of mean/std)\n",
    "- Better predictive power\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Label Definition: Event Horizon = 12 Months\n",
    "\n",
    "**Basel Standard:** PD typically measured over 12-month horizon\n",
    "\n",
    "**Label rule:**\n",
    "```python\n",
    "# Default if: DPD ‚â• 90 days for at least 30 consecutive days in next 12M\n",
    "dpd_90_plus_days = sum(dpd >= 90 for dpd in future_dpd_sequence)\n",
    "event_h12m = 1 if dpd_90_plus_days >= 30 else 0\n",
    "```\n",
    "\n",
    "**Rationale:**\n",
    "- 90 DPD: Industry standard for \"default\"\n",
    "- 30 consecutive days: Avoid transient spikes\n",
    "- 12M horizon: Align with regulatory reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aecc20",
   "metadata": {},
   "source": [
    "## üéì Basel & Regulatory Alignment\n",
    "\n",
    "### Basel Framework Compliance\n",
    "\n",
    "**1. PD (Probability of Default) Estimation**\n",
    "- ‚úÖ 12-month horizon (Basel standard)\n",
    "- ‚úÖ Through-the-cycle (TTC) calibration via Isotonic Regression\n",
    "- ‚úÖ Backtesting with holdout set\n",
    "\n",
    "**2. Key Financial Ratios**\n",
    "- ‚úÖ **ICR (Interest Coverage Ratio):** EBIT / Interest\n",
    "- ‚úÖ **DSCR (Debt Service Coverage Ratio):** (EBITDA - CAPEX) / Debt Service\n",
    "- ‚úÖ **Leverage Ratio:** Total Debt / EBITDA\n",
    "- ‚úÖ **Liquidity Ratio:** Current Assets / Current Liabilities\n",
    "\n",
    "**3. Early Warning Indicators**\n",
    "- ‚úÖ DPD tracking (30, 60, 90+ days)\n",
    "- ‚úÖ Credit limit breach monitoring\n",
    "- ‚úÖ Covenant breach flags\n",
    "- ‚úÖ Cashflow deterioration signals\n",
    "\n",
    "**4. Model Governance**\n",
    "- ‚úÖ **Explainability:** SHAP for transparency\n",
    "- ‚úÖ **Calibration:** Reliability curves\n",
    "- ‚úÖ **Validation:** AUC, KS, Brier on holdout\n",
    "- ‚úÖ **Documentation:** All artifacts saved with metadata\n",
    "\n",
    "---\n",
    "\n",
    "### Risk Appetite Framework\n",
    "\n",
    "**Tier Definitions aligned with Risk Appetite:**\n",
    "\n",
    "| Tier | PD Range | Portfolio Allocation | Risk Appetite |\n",
    "|------|----------|---------------------|---------------|\n",
    "| Green | < 5% | 85% | Accept: Standard monitoring |\n",
    "| Amber | 5-20% | 10% | Tolerate: Enhanced monitoring |\n",
    "| Red | ‚â• 20% | 5% | Mitigate/Exit: Immediate action |\n",
    "\n",
    "**Capacity Management:**\n",
    "- Red tier (5%): Max ~50 customers ‚Üí 5 FTE RM (10 customers/RM)\n",
    "- Amber tier (10%): Max ~100 customers ‚Üí 10 FTE RM (10 customers/RM)\n",
    "- Green tier (85%): Portfolio monitoring only\n",
    "\n",
    "---\n",
    "\n",
    "### Regulatory Reporting\n",
    "\n",
    "**Outputs compatible with:**\n",
    "- **IFRS 9:** Expected Credit Loss (ECL) calculation\n",
    "  - PD √ó LGD √ó EAD = ECL\n",
    "  - Model provides PD component\n",
    "  \n",
    "- **Basel II/III:** Internal Ratings-Based (IRB) approach\n",
    "  - PD model for corporate exposures\n",
    "  - Complement with LGD and EAD models\n",
    "  \n",
    "- **Stress Testing:** Scenario-based PD adjustments\n",
    "  - Re-run model with stressed features\n",
    "  - Example: Revenue shock, Interest rate shock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd156561",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Development & Deployment\n",
    "\n",
    "### Local Development Setup\n",
    "\n",
    "```bash\n",
    "# 1. Clone repository\n",
    "git clone https://github.com/dylanng3/corporate-credit-ews.git\n",
    "cd corporate-credit-ews\n",
    "\n",
    "# 2. Create virtual environment (Python 3.13)\n",
    "python -m venv .venv\n",
    ".venv\\Scripts\\activate  # Windows\n",
    "source .venv/bin/activate  # Linux/Mac\n",
    "\n",
    "# 3. Install dependencies\n",
    "pip install -U pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 4. Run linting\n",
    "make lint\n",
    "\n",
    "# 5. Format code\n",
    "make format\n",
    "\n",
    "# 6. Run tests\n",
    "make test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Testing Strategy\n",
    "\n",
    "**Unit Tests (`tests/test_data.py`):**\n",
    "```python\n",
    "def test_feature_engineering():\n",
    "    \"\"\"Test feature calculation logic\"\"\"\n",
    "    # Load sample data\n",
    "    # Compute features\n",
    "    # Assert feature values are correct\n",
    "\n",
    "def test_model_inference():\n",
    "    \"\"\"Test model scoring pipeline\"\"\"\n",
    "    # Load trained model\n",
    "    # Score test cases\n",
    "    # Assert outputs are valid probabilities [0, 1]\n",
    "```\n",
    "\n",
    "**Integration Tests:**\n",
    "- End-to-end pipeline test\n",
    "- Data ‚Üí Features ‚Üí Model ‚Üí Scores\n",
    "\n",
    "---\n",
    "\n",
    "### Production Deployment Options\n",
    "\n",
    "**Option 1: Batch Scoring (Recommended)**\n",
    "```bash\n",
    "# Cron job: Monthly on last day of month\n",
    "0 23 L * * python src/scoring.py --features <path> --model <path> --asof $(date +%Y-%m-%d) --outdir artifacts/scoring\n",
    "```\n",
    "\n",
    "**Option 2: REST API (Real-time)**\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "model = joblib.load('artifacts/models/model_lgbm.pkl')\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict(features: dict):\n",
    "    X = prepare_features(features)\n",
    "    prob = model.predict_proba(X)[:, 1][0]\n",
    "    tier = assign_tier(prob)\n",
    "    return {'prob': prob, 'tier': tier}\n",
    "```\n",
    "\n",
    "**Option 3: Airflow DAG (Orchestration)**\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "dag = DAG('ews_monthly', schedule_interval='0 0 L * *')\n",
    "\n",
    "generate_features = BashOperator(\n",
    "    task_id='generate_features',\n",
    "    bash_command='python src/feature_engineering.py ...'\n",
    ")\n",
    "\n",
    "score_customers = BashOperator(\n",
    "    task_id='score_customers',\n",
    "    bash_command='python src/scoring.py ...'\n",
    ")\n",
    "\n",
    "generate_features >> score_customers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Model Versioning\n",
    "\n",
    "**Recommended: MLflow or DVC**\n",
    "```bash\n",
    "# Track model with MLflow\n",
    "mlflow.log_model(model, 'lgbm_ews_v1')\n",
    "mlflow.log_metrics({'auc': 0.82, 'ks': 0.45})\n",
    "mlflow.log_artifacts('artifacts/models')\n",
    "\n",
    "# Or version with DVC\n",
    "dvc add artifacts/models/model_lgbm.pkl\n",
    "git add artifacts/models/model_lgbm.pkl.dvc\n",
    "git commit -m \"Model v1.0 - AUC 0.82\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce714d6f",
   "metadata": {},
   "source": [
    "## üìö Future Enhancements\n",
    "\n",
    "### 1. Model Improvements\n",
    "\n",
    "**Advanced Models:**\n",
    "- ‚ú® **XGBoost:** Alternative to LightGBM (may perform better)\n",
    "- ‚ú® **Neural Networks:** TabNet, FT-Transformer for tabular data\n",
    "- ‚ú® **Ensemble:** Stack LightGBM + XGBoost + Logistic Regression\n",
    "\n",
    "**Feature Engineering:**\n",
    "- ‚ú® **Macro variables:** GDP growth, interest rate, sector indices\n",
    "- ‚ú® **Time-series features:** ARIMA residuals, trend slopes\n",
    "- ‚ú® **Text features:** NLP on management discussions, news sentiment\n",
    "- ‚ú® **Network features:** Supply chain relationships, customer concentration\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sources\n",
    "\n",
    "**External Data Integration:**\n",
    "- üìä **Credit Bureau:** Payment history from other banks\n",
    "- üìä **Market Data:** Stock price (if listed), bond spreads\n",
    "- üìä **Alternative Data:** Social media sentiment, web traffic\n",
    "- üìä **Geospatial:** Location risk (flood zones, economic zones)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretability Enhancements\n",
    "\n",
    "**Beyond SHAP:**\n",
    "- üìñ **LIME:** Local surrogate models\n",
    "- üìñ **Counterfactuals:** \"If ICR increased by 0.5, tier would change from Red to Amber\"\n",
    "- üìñ **Rule extraction:** Decision trees from LightGBM for simple rules\n",
    "- üìñ **Narrative generation:** Auto-generate credit memos\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Operational Features\n",
    "\n",
    "**Dashboard (Streamlit/Dash):**\n",
    "```python\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "st.title(\"Corporate Credit EWS Dashboard\")\n",
    "\n",
    "scores = pd.read_csv('artifacts/scoring/ews_scored_latest.csv')\n",
    "\n",
    "# Tier distribution pie chart\n",
    "st.plotly_chart(px.pie(scores, names='tier'))\n",
    "\n",
    "# Customer search\n",
    "customer_id = st.text_input(\"Customer ID\")\n",
    "if customer_id:\n",
    "    customer = scores[scores['customer_id'] == customer_id]\n",
    "    st.metric(\"EWS Score\", customer['score_ews'].iloc[0])\n",
    "    st.metric(\"PD 12M\", f\"{customer['prob_default_12m_calibrated'].iloc[0]:.1%}\")\n",
    "```\n",
    "\n",
    "**Alerting System:**\n",
    "- Email/Slack alerts when customer moves to Red tier\n",
    "- Weekly summary of tier migrations\n",
    "- Covenant breach notifications\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Model Monitoring\n",
    "\n",
    "**Drift Detection:**\n",
    "- **Population Stability Index (PSI):** Track feature distributions\n",
    "- **Model Performance Tracking:** AUC, KS on rolling windows\n",
    "- **Prediction Stability:** Variance of predictions month-over-month\n",
    "\n",
    "**Auto-retraining:**\n",
    "- Trigger retraining if PSI > 0.15 or AUC drops > 5%\n",
    "- A/B test new model vs production model\n",
    "- Gradual rollout with champion-challenger strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055acd37",
   "metadata": {},
   "source": [
    "## üìñ References & Resources\n",
    "\n",
    "### Academic & Industry Papers\n",
    "\n",
    "1. **Basel Committee on Banking Supervision**\n",
    "   - [Basel II: International Convergence of Capital Measurement](https://www.bis.org/publ/bcbs128.htm)\n",
    "   - PD, LGD, EAD estimation frameworks\n",
    "   \n",
    "2. **IFRS 9 - Expected Credit Loss**\n",
    "   - 12-month vs Lifetime PD\n",
    "   - Staging models (Stage 1, 2, 3)\n",
    "\n",
    "3. **Altman Z-Score (1968)**\n",
    "   - Classic credit scoring model for manufacturing firms\n",
    "   - Foundation for many modern models\n",
    "\n",
    "4. **SHAP: Lundberg & Lee (2017)**\n",
    "   - [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)\n",
    "   - Game-theoretic feature attribution\n",
    "\n",
    "---\n",
    "\n",
    "### Tools & Libraries\n",
    "\n",
    "**Python Packages:**\n",
    "- `lightgbm`: Gradient boosting framework\n",
    "- `shap`: Model explainability\n",
    "- `scikit-learn`: ML utilities, calibration\n",
    "- `pandas`, `numpy`: Data manipulation\n",
    "- `matplotlib`, `seaborn`, `plotly`: Visualization\n",
    "\n",
    "**Development:**\n",
    "- `ruff`: Fast Python linter & formatter\n",
    "- `pytest`: Testing framework\n",
    "- `cookiecutter-data-science`: Project template\n",
    "\n",
    "---\n",
    "\n",
    "### Credit Risk Resources\n",
    "\n",
    "**Books:**\n",
    "- *Credit Risk Modeling* by David Lando\n",
    "- *The Credit Scoring Toolkit* by Raymond Anderson\n",
    "- *Machine Learning for Asset Managers* by Marcos L√≥pez de Prado\n",
    "\n",
    "**Online Courses:**\n",
    "- Coursera: *Credit Risk Modeling in Python*\n",
    "- Udemy: *Credit Risk Analytics with Python*\n",
    "\n",
    "**Websites:**\n",
    "- [Risk.net](https://www.risk.net) - Industry news\n",
    "- [Kaggle Credit Risk Datasets](https://www.kaggle.com/search?q=credit+risk)\n",
    "\n",
    "---\n",
    "\n",
    "### Contact & Support\n",
    "\n",
    "**Project Maintainer:** Duong N.C.K  \n",
    "**Repository:** [github.com/dylanng3/corporate-credit-ews](https://github.com/dylanng3/corporate-credit-ews)  \n",
    "**License:** MIT License\n",
    "\n",
    "**For questions or contributions:**\n",
    "- Open an issue on GitHub\n",
    "- Submit a pull request\n",
    "- Email: [contact info if available]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6545e",
   "metadata": {},
   "source": [
    "## üéØ Summary & Conclusion\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A **production-ready Early Warning System** for corporate credit risk with:\n",
    "\n",
    "‚úÖ **End-to-end ML pipeline** (data ‚Üí features ‚Üí model ‚Üí scoring ‚Üí explainability)  \n",
    "‚úÖ **Basel-aligned methodology** (12M PD, ICR, DSCR, leverage ratios)  \n",
    "‚úÖ **LightGBM + Isotonic Calibration** (AUC ~0.75-0.85, well-calibrated probabilities)  \n",
    "‚úÖ **SHAP explainability** (global + local feature importance)  \n",
    "‚úÖ **3-tier risk classification** (Green/Amber/Red with actionable recommendations)  \n",
    "‚úÖ **Production scoring pipeline** (batch scoring with thresholds)  \n",
    "‚úÖ **Comprehensive artifacts** (models, scores, plots, metadata)\n",
    "\n",
    "---\n",
    "\n",
    "### Key Strengths\n",
    "\n",
    "1. **Comprehensive Feature Engineering**\n",
    "   - Financial ratios (ICR, DSCR, Leverage, CCC)\n",
    "   - Behavioral signals (DPD, utilization, trends)\n",
    "   - Cashflow monitoring\n",
    "   - Covenant breach tracking\n",
    "   - Sector-size normalization\n",
    "\n",
    "2. **Model Quality**\n",
    "   - Handles class imbalance with `scale_pos_weight`\n",
    "   - Isotonic calibration for reliable probabilities\n",
    "   - SHAP for transparency and trust\n",
    "\n",
    "3. **Business Integration**\n",
    "   - Clear tier definitions aligned with risk appetite\n",
    "   - Actionable recommendations for RM team\n",
    "   - Monthly scoring cadence\n",
    "   - Customer-level explanations\n",
    "\n",
    "4. **Code Quality**\n",
    "   - Modular design (separate scripts for each stage)\n",
    "   - Type hints and documentation\n",
    "   - CLI interfaces for all scripts\n",
    "   - Cookiecutter-data-science template\n",
    "\n",
    "---\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "**Model Performance:**\n",
    "- ‚úÖ AUC-ROC ‚â• 0.75\n",
    "- ‚úÖ PR-AUC ‚â• 0.40\n",
    "- ‚úÖ KS ‚â• 0.35\n",
    "- ‚úÖ Brier Score ‚â§ 0.10\n",
    "\n",
    "**Business Impact:**\n",
    "- ‚úÖ Early identification of high-risk customers\n",
    "- ‚úÖ Reduction in unexpected defaults\n",
    "- ‚úÖ Efficient RM resource allocation\n",
    "- ‚úÖ Regulatory compliance (Basel, IFRS 9)\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Validation:** Run on real historical data\n",
    "2. **Backtesting:** Validate predictions against actual defaults\n",
    "3. **Integration:** Connect to core banking system for live data feeds\n",
    "4. **Monitoring:** Set up drift detection and performance tracking\n",
    "5. **Iteration:** Refine features and model based on feedback\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook serves as the complete documentation for the Corporate Credit EWS project.**  \n",
    "**For hands-on experimentation, run the pipeline commands in your terminal or create interactive cells below.**\n",
    "\n",
    "üöÄ **Happy modeling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
